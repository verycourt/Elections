{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# coding: utf-8\n",
    "\n",
    "import sys\n",
    "import requests\n",
    "from datetime import date, datetime, timedelta\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "if sys.version_info[0] == 2:\n",
    "    import urllib2 as ul # Python2\n",
    "else:\n",
    "    import urllib.request as ul # Python3\n",
    "import json\n",
    "\n",
    "\n",
    "def FacebookPageData(page_id, access_token):\n",
    "    \n",
    "    # construct the URL string\n",
    "    base = 'https://graph.facebook.com/v2.8'\n",
    "    node = '/' + page_id\n",
    "    parameters = '/?access_token=%s&fields=name,talking_about_count,fan_count' % access_token\n",
    "    url = base + node + parameters\n",
    "    \n",
    "    # retrieve data\n",
    "    response = ul.urlopen(url)\n",
    "    data = json.loads(response.read().decode('utf-8'))\n",
    "    \n",
    "    print('Facebook page :', data['name'])\n",
    "    return [int(data[metric]) for metric in ['fan_count', 'talking_about_count']]\n",
    "\n",
    "def YoutubePageData(page_id, access_token):\n",
    "    base = 'https://www.googleapis.com/youtube/v3/channels'\n",
    "    parameters = '?part=statistics&id=' + page_id + '&key=' + access_token\n",
    "    url = base + parameters\n",
    "    \n",
    "    # retrieve data\n",
    "    response = ul.urlopen(url)\n",
    "    data = json.loads(response.read().decode('utf-8'))\n",
    "    statistics = data['items'][0]['statistics']\n",
    "\n",
    "    return [int(statistics[metric]) for metric in ['subscriberCount', 'viewCount', 'videoCount']]\n",
    "\n",
    "def YoutubeVideosData(page_id, access_token):\n",
    "    base = 'https://www.googleapis.com/youtube/v3/search'\n",
    "    parameters = '?order=date&part=snippet&channelId=' + page_id + '&maxResults=10&key=' + access_token\n",
    "    url = base + parameters\n",
    "    \n",
    "    # retrieve list of the most recently published videos on the channel (10 or less)\n",
    "    response = ul.urlopen(url)\n",
    "    data = json.loads(response.read().decode('utf-8'))\n",
    "    videoIds = [e['id']['videoId'] for e in data['items'] if 'videoId' in e['id']]\n",
    "    \n",
    "    base = 'https://www.googleapis.com/youtube/v3/videos'\n",
    "    parameters = '?part=statistics&id=' + ','.join(videoIds) + '&key=' + access_token\n",
    "    url = base + parameters\n",
    "    \n",
    "    response = ul.urlopen(url)\n",
    "    data = json.loads(response.read().decode('utf-8'))\n",
    "    keys = data['items'][0]['statistics'].keys() # list of metrics\n",
    "    n = len(data['items'])\n",
    "    \n",
    "    # Construction du dictionnaire des valeurs moyennes pour chaque clé sur les vidéos analysées\n",
    "    videoStats = {key: int(round(sum([int(e['statistics'][key]) for e in data['items']]) / n)) for key in keys}\n",
    "    \n",
    "    print('Getting average metrics for the latest', n, 'videos of the channel')\n",
    "\n",
    "    return [videoStats[metric] for metric in ['viewCount', 'likeCount', 'dislikeCount']]\n",
    "\n",
    "def get_metrics():\n",
    "    df = pd.DataFrame()\n",
    "    print('Maj du', today)\n",
    "\n",
    "    for candidate in accounts:\n",
    "        print('-' * 20)\n",
    "        print(candidate)\n",
    "        print('-' * 20)\n",
    "\n",
    "        stats = {}\n",
    "        try: # Twitter : [tweets, followers]\n",
    "            print('Analyzing Twitter account', accounts[candidate][2])\n",
    "            soup = BeautifulSoup(requests.get('https://twitter.com/' + accounts[candidate][2] + '?lang=en').text, 'lxml')\n",
    "            stats_tw = [int(tag.attrs['title'].replace(',', '').split(' ')[0])\n",
    "                        for tag in soup.find_all(class_='ProfileNav-stat', limit=3) if 'title' in tag.attrs]\n",
    "        except:\n",
    "            stats_tw = ['-', '-', '-']\n",
    "            print('Profil Twitter : une erreur est survenue...')\n",
    "\n",
    "        _, _, stats['0_tw_followers'] = stats_tw\n",
    "\n",
    "        if accounts[candidate][0] is not None:\n",
    "            print('Scanning Youtube Channel')\n",
    "            try: # Youtube [abonnés, total vues, nombre de vidéos]\n",
    "                stats_yt = YoutubePageData(accounts[candidate][0], google_key)\n",
    "            except:\n",
    "                stats_yt = ['-', '-', '-']\n",
    "                print('Page Youtube : une erreur est survenue...')\n",
    "            try: # Youtube [moyenne vues 10 vidéos, moyenne likes 10 vidéos, moyenne dislikes 10 vidéos]\n",
    "                stats_yt2 = YoutubeVideosData(accounts[candidate][0], google_key)\n",
    "            except:\n",
    "                stats_yt2 = ['-', '-', '-']\n",
    "                print('Vidéos Youtube : une erreur est survenue...')\n",
    "        else:\n",
    "            print('No Youtube Channel')\n",
    "            stats_yt, stats_yt2 = ['-', '-', '-'], ['-', '-', '-']\n",
    "\n",
    "        stats['2_yt_subscribers'], _, _ = stats_yt\n",
    "        _, stats['3_yt_like_count'], stats['4_yt_dislike_count'] = stats_yt2\n",
    "\n",
    "        try:\n",
    "            pass\n",
    "            #stats['4_yt_reaction_rate'] = round((float(stats_yt2[1] + stats_yt2[2]) / stats_yt2[0]) * 100, 1)\n",
    "            #stats['5_yt_satisfaction_rate'] = round((float(stats_yt2[1]) / (stats_yt2[2] + stats_yt2[1])) * 100, 1)\n",
    "        except:\n",
    "            pass\n",
    "            #stats['4_yt_reaction_rate'] = '-'\n",
    "            #stats['5_yt_satisfaction_rate'] = '-'\n",
    "\n",
    "        try: # Facebook : [likes, people talking about this]\n",
    "            stats_fb = FacebookPageData(accounts[candidate][1], access_token)\n",
    "        except:\n",
    "            stats_fb = ['-', '-']\n",
    "            print('Page Facebook : une erreur est survenue...')\n",
    "\n",
    "        stats['6_fb_likes'], stats['7_fb_talking_about'] = stats_fb\n",
    "\n",
    "        print()\n",
    "        print(stats)\n",
    "\n",
    "        # ajout de la ligne du candidat dans le dataframe\n",
    "        rec = pd.DataFrame([stats.values()], columns=stats.keys(), index=[candidate])\n",
    "        df = df.append(rec, verify_integrity=False)\n",
    "\n",
    "    # ajout de la colonne des mentions twitter sur 3 jours\n",
    "    #tweet_data = pd.read_json('/var/www/html/decompte/popcontest.json', orient='column')\n",
    "    tweet_data = pd.read_json('popcontest.json', orient='column')\n",
    "    names, counts = [e['name'] for e in tweet_data['children']], [e['size'] for e in tweet_data['children']]\n",
    "    tweet_df = pd.DataFrame(counts, columns=['1_tw_mentions'], index=names)\n",
    "    tweet_df.fillna(value='-', inplace=True)\n",
    "\n",
    "    df = pd.concat([df, tweet_df], axis=1, join_axes=[df.index])\n",
    "    return df\n",
    "\n",
    "def save_metrics(df, timestamp=today):\n",
    "    # sauvegarde des colonnes du dataframe dans les différents .json\n",
    "    for metric in df:\n",
    "        try:\n",
    "            current_df = pd.read_json(path + metric + '.json', orient='split')\n",
    "            \n",
    "            if timestamp in current_df:\n",
    "                print('Fichier json deja a jour pour', metric)\n",
    "                continue\n",
    "                \n",
    "            current_df = pd.concat([current_df, df[metric]], axis=1)\n",
    "            \n",
    "        except ValueError: # si le fichier n'existe pas\n",
    "            current_df = pd.DataFrame(df[metric], columns=[metric], index=[df.index])\n",
    "\n",
    "        current_df.rename(columns={metric:timestamp}, inplace=True)\n",
    "        current_df.fillna(value='-', inplace=True)\n",
    "        current_df.to_json(path + metric + '.json', orient='split')\n",
    "        print('Data saved as ' + path + metric + '.json')\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# {Candidat : [Chaine Youtube, Compte Facebook, Compte Twitter]}\n",
    "accounts = {#'Alliot-Marie': [None, 'MAlliotMarie', 'MAlliotMarie'],\n",
    "           #'Arthaud': ['UCZsh-MrJftAOP_-ZgRgLScw', 'nathaliearthaud', 'n_arthaud'],\n",
    "           #'Bayrou': [None, 'bayrou', 'bayrou'],\n",
    "           #'Cheminade': ['UCCPw8MX-JcsiTzItY-qq1Fg', 'Jcheminade', 'Jcheminade'],\n",
    "           #'Dupont-Aignan': ['UCfA5DnCDX3Ixy5QOAMGtBlA', 'nicolasdupontaignan', 'dupontaignan'],\n",
    "           'Fillon': ['UCp1R4BFJrKw34PfUc3GDLkw', 'FrancoisFillon', 'francoisfillon'],\n",
    "           'Hamon': ['UCcMryUp6ME3BvP2alkS1dKg', 'hamonbenoit', 'benoithamon'],\n",
    "           #'Jadot': ['UCsUMhb2ygeTSS2mXLTIDHMQ', 'yannick.jadot', 'yjadot'],\n",
    "           'Le Pen': ['UCU3z3px1_RCqYBwrs8LJVWg', 'MarineLePen', 'MLP_officiel'],\n",
    "           'Macron': ['UCJw8np695wqWOaKVhFjkRyg', 'EmmanuelMacron', 'emmanuelmacron'],\n",
    "           'Mélenchon': ['UCk-_PEY3iC6DIGJKuoEe9bw', 'JLMelenchon', 'JLMelenchon'],\n",
    "           #'Poutou': [None, 'poutou.philippe', 'PhilippePoutou']\n",
    "}\n",
    "\n",
    "app_id = \"615202351999343\"\n",
    "app_secret = \"ea787efd843d1de746817ec6e9bf7e94\"\n",
    "access_token = app_id + \"|\" + app_secret\n",
    "google_key = 'AIzaSyBkRrj_kFDUv-T76CJaI3Pd-g3v7UY4GMA'\n",
    "\n",
    "today = (datetime.utcnow() + timedelta(hours=1)).date()\n",
    "fname = str(today) + '.json'\n",
    "path = 'data/' # save path\n",
    "#path = '/var/www/html/duel/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maj du 2017-03-14\n",
      "--------------------\n",
      "Fillon\n",
      "--------------------\n",
      "Analyzing Twitter account francoisfillon\n",
      "Scanning Youtube Channel\n",
      "Getting average metrics for the latest 10 videos of the channel\n",
      "Facebook page : François Fillon\n",
      "\n",
      "{'6_fb_likes': 305980, '0_tw_followers': 454691, '4_yt_dislike_count': 41, '7_fb_talking_about': 93980, '2_yt_subscribers': 4579, '3_yt_like_count': 151}\n",
      "--------------------\n",
      "Le Pen\n",
      "--------------------\n",
      "Analyzing Twitter account MLP_officiel\n",
      "Scanning Youtube Channel\n",
      "Getting average metrics for the latest 10 videos of the channel\n",
      "Facebook page : Marine Le Pen\n",
      "\n",
      "{'6_fb_likes': 1250089, '0_tw_followers': 1324117, '4_yt_dislike_count': 14, '7_fb_talking_about': 191624, '2_yt_subscribers': 15762, '3_yt_like_count': 238}\n",
      "--------------------\n",
      "Mélenchon\n",
      "--------------------\n",
      "Analyzing Twitter account JLMelenchon\n",
      "Scanning Youtube Channel\n",
      "Getting average metrics for the latest 10 videos of the channel\n",
      "Facebook page : Jean-Luc Mélenchon\n",
      "\n",
      "{'6_fb_likes': 681838, '0_tw_followers': 992425, '4_yt_dislike_count': 59, '7_fb_talking_about': 135623, '2_yt_subscribers': 232593, '3_yt_like_count': 5879}\n",
      "--------------------\n",
      "Macron\n",
      "--------------------\n",
      "Analyzing Twitter account emmanuelmacron\n",
      "Scanning Youtube Channel\n",
      "Getting average metrics for the latest 10 videos of the channel\n",
      "Facebook page : Emmanuel Macron\n",
      "\n",
      "{'6_fb_likes': 209454, '0_tw_followers': 559731, '4_yt_dislike_count': 676, '7_fb_talking_about': 55332, '2_yt_subscribers': 10592, '3_yt_like_count': 406}\n",
      "--------------------\n",
      "Hamon\n",
      "--------------------\n",
      "Analyzing Twitter account benoithamon\n",
      "Scanning Youtube Channel\n",
      "Getting average metrics for the latest 10 videos of the channel\n",
      "Facebook page : Benoît Hamon\n",
      "\n",
      "{'6_fb_likes': 139625, '0_tw_followers': 343109, '4_yt_dislike_count': 11, '7_fb_talking_about': 47938, '2_yt_subscribers': 5577, '3_yt_like_count': 47}\n",
      "Fichier json deja a jour pour 6_fb_likes\n",
      "Fichier json deja a jour pour 0_tw_followers\n",
      "Fichier json deja a jour pour 4_yt_dislike_count\n",
      "Fichier json deja a jour pour 7_fb_talking_about\n",
      "Fichier json deja a jour pour 2_yt_subscribers\n",
      "Fichier json deja a jour pour 3_yt_like_count\n",
      "Fichier json deja a jour pour 1_tw_mentions\n"
     ]
    }
   ],
   "source": [
    "save_metrics(get_metrics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2017-02-13 00:00:00</th>\n",
       "      <th>2017-02-14 00:00:00</th>\n",
       "      <th>2017-02-15 00:00:00</th>\n",
       "      <th>2017-02-16 00:00:00</th>\n",
       "      <th>2017-02-17 00:00:00</th>\n",
       "      <th>2017-02-18 00:00:00</th>\n",
       "      <th>2017-02-19 00:00:00</th>\n",
       "      <th>2017-02-20 00:00:00</th>\n",
       "      <th>2017-02-21 00:00:00</th>\n",
       "      <th>2017-02-22 00:00:00</th>\n",
       "      <th>...</th>\n",
       "      <th>2017-03-05 00:00:00</th>\n",
       "      <th>2017-03-06 00:00:00</th>\n",
       "      <th>2017-03-07 00:00:00</th>\n",
       "      <th>2017-03-08 00:00:00</th>\n",
       "      <th>2017-03-09 00:00:00</th>\n",
       "      <th>2017-03-10 00:00:00</th>\n",
       "      <th>2017-03-11 00:00:00</th>\n",
       "      <th>2017-03-12 00:00:00</th>\n",
       "      <th>2017-03-13 00:00:00</th>\n",
       "      <th>2017-03-14 00:00:00</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Fillon</th>\n",
       "      <td>422547</td>\n",
       "      <td>423181</td>\n",
       "      <td>424011</td>\n",
       "      <td>425149</td>\n",
       "      <td>426014</td>\n",
       "      <td>426789</td>\n",
       "      <td>427675</td>\n",
       "      <td>428638</td>\n",
       "      <td>429347</td>\n",
       "      <td>430143</td>\n",
       "      <td>...</td>\n",
       "      <td>445218</td>\n",
       "      <td>447365</td>\n",
       "      <td>448673</td>\n",
       "      <td>449698</td>\n",
       "      <td>450722</td>\n",
       "      <td>451641</td>\n",
       "      <td>452544</td>\n",
       "      <td>453580</td>\n",
       "      <td>454327</td>\n",
       "      <td>454690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hamon</th>\n",
       "      <td>326052</td>\n",
       "      <td>326739</td>\n",
       "      <td>327730</td>\n",
       "      <td>328988</td>\n",
       "      <td>329775</td>\n",
       "      <td>330478</td>\n",
       "      <td>331323</td>\n",
       "      <td>331974</td>\n",
       "      <td>332387</td>\n",
       "      <td>332865</td>\n",
       "      <td>...</td>\n",
       "      <td>338944</td>\n",
       "      <td>339670</td>\n",
       "      <td>340218</td>\n",
       "      <td>340815</td>\n",
       "      <td>341421</td>\n",
       "      <td>342218</td>\n",
       "      <td>342576</td>\n",
       "      <td>342768</td>\n",
       "      <td>342946</td>\n",
       "      <td>343109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Le Pen</th>\n",
       "      <td>1272008</td>\n",
       "      <td>1272977</td>\n",
       "      <td>1274053</td>\n",
       "      <td>1275497</td>\n",
       "      <td>1276784</td>\n",
       "      <td>1278058</td>\n",
       "      <td>1279544</td>\n",
       "      <td>1281410</td>\n",
       "      <td>1284466</td>\n",
       "      <td>1287438</td>\n",
       "      <td>...</td>\n",
       "      <td>1309959</td>\n",
       "      <td>1312222</td>\n",
       "      <td>1313884</td>\n",
       "      <td>1315306</td>\n",
       "      <td>1316938</td>\n",
       "      <td>1318486</td>\n",
       "      <td>1319819</td>\n",
       "      <td>1321599</td>\n",
       "      <td>1323344</td>\n",
       "      <td>1324114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Macron</th>\n",
       "      <td>57757</td>\n",
       "      <td>57998</td>\n",
       "      <td>496473</td>\n",
       "      <td>499164</td>\n",
       "      <td>501489</td>\n",
       "      <td>503556</td>\n",
       "      <td>505678</td>\n",
       "      <td>508232</td>\n",
       "      <td>510271</td>\n",
       "      <td>513332</td>\n",
       "      <td>...</td>\n",
       "      <td>539827</td>\n",
       "      <td>542253</td>\n",
       "      <td>544300</td>\n",
       "      <td>546901</td>\n",
       "      <td>549529</td>\n",
       "      <td>551699</td>\n",
       "      <td>553740</td>\n",
       "      <td>556027</td>\n",
       "      <td>558547</td>\n",
       "      <td>559729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mélenchon</th>\n",
       "      <td>963525</td>\n",
       "      <td>964205</td>\n",
       "      <td>965153</td>\n",
       "      <td>966236</td>\n",
       "      <td>967289</td>\n",
       "      <td>968269</td>\n",
       "      <td>969236</td>\n",
       "      <td>970551</td>\n",
       "      <td>971523</td>\n",
       "      <td>972501</td>\n",
       "      <td>...</td>\n",
       "      <td>984215</td>\n",
       "      <td>985415</td>\n",
       "      <td>986502</td>\n",
       "      <td>987424</td>\n",
       "      <td>988388</td>\n",
       "      <td>989204</td>\n",
       "      <td>990033</td>\n",
       "      <td>991081</td>\n",
       "      <td>991975</td>\n",
       "      <td>992423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           2017-02-13  2017-02-14  2017-02-15  2017-02-16  2017-02-17  \\\n",
       "Fillon         422547      423181      424011      425149      426014   \n",
       "Hamon          326052      326739      327730      328988      329775   \n",
       "Le Pen        1272008     1272977     1274053     1275497     1276784   \n",
       "Macron          57757       57998      496473      499164      501489   \n",
       "Mélenchon      963525      964205      965153      966236      967289   \n",
       "\n",
       "           2017-02-18  2017-02-19  2017-02-20  2017-02-21  2017-02-22  \\\n",
       "Fillon         426789      427675      428638      429347      430143   \n",
       "Hamon          330478      331323      331974      332387      332865   \n",
       "Le Pen        1278058     1279544     1281410     1284466     1287438   \n",
       "Macron         503556      505678      508232      510271      513332   \n",
       "Mélenchon      968269      969236      970551      971523      972501   \n",
       "\n",
       "              ...      2017-03-05  2017-03-06  2017-03-07  2017-03-08  \\\n",
       "Fillon        ...          445218      447365      448673      449698   \n",
       "Hamon         ...          338944      339670      340218      340815   \n",
       "Le Pen        ...         1309959     1312222     1313884     1315306   \n",
       "Macron        ...          539827      542253      544300      546901   \n",
       "Mélenchon     ...          984215      985415      986502      987424   \n",
       "\n",
       "           2017-03-09  2017-03-10  2017-03-11  2017-03-12  2017-03-13  \\\n",
       "Fillon         450722      451641      452544      453580      454327   \n",
       "Hamon          341421      342218      342576      342768      342946   \n",
       "Le Pen        1316938     1318486     1319819     1321599     1323344   \n",
       "Macron         549529      551699      553740      556027      558547   \n",
       "Mélenchon      988388      989204      990033      991081      991975   \n",
       "\n",
       "           2017-03-14  \n",
       "Fillon         454690  \n",
       "Hamon          343109  \n",
       "Le Pen        1324114  \n",
       "Macron         559729  \n",
       "Mélenchon      992423  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.read_json('data/6_fb_likes.json', orient='split')\n",
    "# a=a.sort_index(axis=0)\n",
    "# a\n",
    "b = pd.read_json('data/0_tw_followers.json', orient='split')\n",
    "b\n",
    "# df = pd.concat([b,a], axis=1)\n",
    "# df.to_json('data/1_tw_mentions.json', orient='split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/ ['old'] 0_tw_followers.json\n",
      "data/ ['old'] 1_tw_mentions.json\n",
      "data/ ['old'] 2_yt_subscribers.json\n",
      "data/ ['old'] 3_yt_like_count.json\n",
      "data/ ['old'] 3_yt_views_avg.json\n",
      "data/ ['old'] 4_yt_likes_rate.json\n",
      "data/ ['old'] 4_yt_reaction_rate.json\n",
      "data/ ['old'] 5_yt_dislikes_rate.json\n",
      "data/ ['old'] 5_yt_satisfaction_rate.json\n",
      "data/ ['old'] 6_fb_likes.json\n",
      "data/ ['old'] 7_fb_talking_about.json\n",
      "data/old [] 2017-02-14.json\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-6c53666177cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'split'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'split'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\io\\json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines)\u001b[0m\n\u001b[1;32m    279\u001b[0m         obj = FrameParser(json, orient, dtype, convert_axes, convert_dates,\n\u001b[1;32m    280\u001b[0m                           \u001b[0mkeep_default_dates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m                           date_unit).parse()\n\u001b[0m\u001b[1;32m    282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'series'\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\io\\json.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\io\\json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    569\u001b[0m                            for k, v in compat.iteritems(loads(\n\u001b[1;32m    570\u001b[0m                                \u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m                                precise_float=self.precise_float)))\n\u001b[0m\u001b[1;32m    572\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_keys_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "import os\n",
    "folder = 'data/'\n",
    "for tex, subdir, files in os.walk(folder):\n",
    "    for file in files:\n",
    "        print(tex, subdir, file)\n",
    "#         save_metrics(pd.read_json(folder + file, orient='split'),\n",
    "#                      date=datetime.strptime(file[:-5], '%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
