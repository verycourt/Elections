{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymongo as pym\n",
    "import nltk.data\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import stop_words\n",
    "from nltk.stem import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'je', 'la', 'le', 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'été', 'étée', 'étées', 'étés', 'étant', 'étante', 'étants', 'étantes', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent']\n",
      "---\n",
      "['a', 'ai', 'aie', 'aient', 'aies', 'ait', 'alors', 'as', 'au', 'aucun', 'aura', 'aurai', 'auraient', 'aurais', 'aurait', 'auras', 'aurez', 'auriez', 'aurions', 'aurons', 'auront', 'aussi', 'autre', 'aux', 'avaient', 'avais', 'avait', 'avant', 'avec', 'avez', 'aviez', 'avions', 'avoir', 'avons', 'ayant', 'ayez', 'ayons', 'bon', 'car', 'ce', 'ceci', 'cela', 'ces', 'cet', 'cette', 'ceux', 'chaque', 'ci', 'comme', 'comment', 'd', 'dans', 'de', 'dedans', 'dehors', 'depuis', 'des', 'deux', 'devoir', 'devrait', 'devrez', 'devriez', 'devrions', 'devrons', 'devront', 'dois', 'doit', 'donc', 'dos', 'droite', 'du', 'dès', 'début', 'dù', 'elle', 'elles', 'en', 'encore', 'es', 'est', 'et', 'eu', 'eue', 'eues', 'eurent', 'eus', 'eusse', 'eussent', 'eusses', 'eussiez', 'eussions', 'eut', 'eux', 'eûmes', 'eût', 'eûtes', 'faire', 'fais', 'faisez', 'fait', 'faites', 'fois', 'font', 'force', 'furent', 'fus', 'fusse', 'fussent', 'fusses', 'fussiez', 'fussions', 'fut', 'fûmes', 'fût', 'fûtes', 'haut', 'hors', 'ici', 'il', 'ils', 'j', 'je', 'juste', 'l', 'la', 'le', 'les', 'leur', 'leurs', 'lui', 'là', 'm', 'ma', 'maintenant', 'mais', 'me', 'mes', 'moi', 'moins', 'mon', 'mot', 'même', 'n', 'ne', 'ni', 'nom', 'nommé', 'nommée', 'nommés', 'nos', 'notre', 'nous', 'nouveau', 'nouveaux', 'on', 'ont', 'ou', 'où', 'par', 'parce', 'parole', 'pas', 'personne', 'personnes', 'peu', 'peut', 'plupart', 'pour', 'pourquoi', 'qu', 'quand', 'que', 'quel', 'quelle', 'quelles', 'quels', 'qui', 'sa', 'sans', 'se', 'sera', 'serai', 'seraient', 'serais', 'serait', 'seras', 'serez', 'seriez', 'serions', 'serons', 'seront', 'ses', 'seulement', 'si', 'sien', 'soi', 'soient', 'sois', 'soit', 'sommes', 'son', 'sont', 'sous', 'soyez', 'soyons', 'suis', 'sujet', 'sur', 't', 'ta', 'tandis', 'te', 'tellement', 'tels', 'tes', 'toi', 'ton', 'tous', 'tout', 'trop', 'très', 'tu', 'un', 'une', 'valeur', 'voient', 'vois', 'voit', 'vont', 'vos', 'votre', 'vous', 'vu', 'y', 'à', 'ça', 'étaient', 'étais', 'était', 'étant', 'état', 'étiez', 'étions', 'été', 'étés', 'êtes', 'être']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('french'))\n",
    "print('---')\n",
    "print(stop_words.get_stop_words('fr'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'serait', 'fussiez', 'pour', 'eu', 'au', 'as', 'en', 'fût', 'fusse', 'été', 'u', 'v', 'eusses', 'fussions', 'aviez', 'seras', 'ds', 'eûtes', 'nos', 'on', 'ni', 'même', 'sommes', 'ayons', 'es', 'n', 'ma', 'h', 'sa', 'eux', 'notre', 'eue', 'dans', 'si', 'ns', 'aurai', 'avait', 'sera', 'fûtes', 'que', 'ac', 'étants', 'ton', 'eussions', 'lui', 'i', 'avais', 'et', 'nn', 'ayants', 'la', 'je', 'avons', 'qui', 'serions', 'ce', 'ca', 'ayant', 'soyons', 'vu', 'avions', '$', 'une', 't', 'étantes', 'w', 'mais', 'serai', 'mon', 'pr', 'tes', 'k', 'fussent', 'aurons', 'qu', 'fus', 'ayantes', 'seront', 's', 'nous', 'serais', 'de', 'seriez', 'fusses', 'eût', 'gd', 'ta', 'étée', 'étant', 'aie', 'le', 'c', 'ayante', 'ses', 'auras', 'eus', 'z', 'aient', 'étante', 'auraient', 'eussent', 'vous', 'gt', 'te', ' ', 'étions', 'via', 'auriez', 'l', 'furent', 'aux', 'soient', 'm', 'ai', 'o', 'tt', 'leur', 'étaient', 'tu', 'me', 'ayez', 'q', 'sur', 'se', 'les', 'fûmes', 'aura', 'y', 'vos', 'sont', 'ss', 'suis', 'seraient', 'g', 'sois', '``', 'r', 'à', 'fut', 'aies', 'avec', 'toi', 'j', 'serons', 'étés', 'elle', 'mm', 'aurez', 'étiez', 'eûmes', 'était', 'votre', 'e', 'soyez', 'un', 'eues', 'eussiez', 'étais', 'son', 'des', 'ou', 'b', 'amp', 'aurais', 'eurent', 'mes', 'étées', 'x', 'rt', 'gds', 'du', 'par', 'est', 'soit', 'ces', 'serez', 'a', 'il', 'auront', 'p', 'f', 'aurions', 'eut', 'avez', '^', 'êtes', 'avaient', 'aurait', 'moi', 'vs', 'qd', 'd', 'pas', 'ne', 'ait', 'ont', 'eusse'}\n"
     ]
    }
   ],
   "source": [
    "client = pym.MongoClient('localhost', 27017)\n",
    "collection = client.tweet.cleaned\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "stops = set(stopwords.words('french') +\n",
    "            ['rt','ds','qd','ss','ns','vs','nn','amp','gt','gd','gds','tt','pr','ac','mm', 'qu',\n",
    "            '``', 'ni', 'ca', 'le', 'les', ' ', 'si', '$', '^', 'via', 'ils'] +\n",
    "           list('abcdefghijklmnopqrstuvwxyz'))\n",
    "print(stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getCleanTweetsByCandidates(collection):\n",
    "    #tweets = collection.find({'candidat':candidat},{'_id':False})\n",
    "    tweets = collection.find(filter={'text':{'$exists':True}}, projection={'_id':False})\n",
    "    listTweets, listCandidats, listSentiments = [], [], []\n",
    "    for t in tweets: \n",
    "        # mot tronqué\n",
    "        t['text'] = re.sub(r'\\w*…', '', t['text'])\n",
    "        \n",
    "        # caracteres speciaux, url et rt\n",
    "        t['text'] = re.sub(r'\\xad', '-', re.sub(r'\\n', ' ',\n",
    "                        re.sub(r'\\W*(?!\\S)', '', re.sub(r'(?:htt)\\S*', '',\n",
    "                        re.sub(r'^rt.*: ', '', re.sub(r'\\d', '', \n",
    "                        re.sub(r',;:!?\\.\\/\\*(){}', '', string=t['text'])))))))\n",
    "        \n",
    "        t['text'] = re.sub('|'.join(['@', '#', 'ن', '%', '£', '€']), '', t['text'])\n",
    "        t['text'] = re.sub('|'.join(['’', '_', '-', '\\'', '\\.', '/']), ' ', t['text'])\n",
    "        \n",
    "        # accents\n",
    "        t['text'] = re.sub('|'.join('Ééèêë'), 'e', t['text'])\n",
    "        t['text'] = re.sub('|'.join('àâä'), 'a', t['text'])\n",
    "        t['text'] = re.sub('|'.join('ç'), 'c', t['text'])\n",
    "        t['text'] = re.sub('|'.join('œ'), 'oe', t['text'])\n",
    "        t['text'] = re.sub('|'.join('Ôôö'), 'o', t['text'])\n",
    "        t['text'] = re.sub('|'.join('îï'), 'i', t['text'])\n",
    "        t['text'] = re.sub('|'.join('ùû'), 'u', t['text'])\n",
    "        \n",
    "        # apostrophes\n",
    "        t['text'] = re.sub('|'.join([elem + '\\'' for elem in 'cdjlmnst']), '', t['text'])\n",
    "        \n",
    "        t['text'] = tokenizer.tokenize(t['text'])\n",
    "        t['text'] = [token for token in t['text'] if token not in stops]\n",
    "\n",
    "        while '' in t['text']:\n",
    "            t['text'].pop('')\n",
    "            \n",
    "        listTweets.append(t['text'])\n",
    "        \n",
    "        try:\n",
    "            listCandidats.append(t['candidat'])\n",
    "        except:\n",
    "            listCandidats.append(None)\n",
    "        \n",
    "        try:\n",
    "            listSentiments.append(t['sentiment'])\n",
    "        except:\n",
    "            listSentiments.append(None)\n",
    "        \n",
    "    df = pd.DataFrame()\n",
    "    df['text'], df['candidat'], df['sentiment'] = listTweets, listCandidats, listSentiments\n",
    "    return df\n",
    "\n",
    "def vectorize(tokenizedCorpus):\n",
    "    vectorizer = TfidfVectorizer(max_df = 0.8, min_df=0.0005)\n",
    "    X = vectorizer.fit_transform(tokenizedCorpus)\n",
    "    return X\n",
    "\n",
    "def getSentiments() : \n",
    "    df = getCleanTweetsByCandidates(collection)\n",
    "    tfidfmat = vectorize(df['text'].apply(' '.join))\n",
    "    print(tfidfmat.shape)\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(tfidfmat[::2], df.ix[::2, 'sentiment'])\n",
    "    predictions = nb.predict(tfidfmat[1::2])\n",
    "    accuracy = np.sum(predictions != df.ix[1::2, 'sentiment']) / len(df.ix[1::2, 'sentiment'])\n",
    "    print(\"accuracy of the sklearn naive bayes : \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3503"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = getCleanTweetsByCandidates(collection)\n",
    "corpus.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['couvertures', 'valeurs', 'toujours', 'ete', 'pathetiques', 'ils', 'creusent', 'encore', 'aller', 'plus', 'bas']\n",
      "['voudrait', 'girouette', 'comme', 'president', 'stopmacron']\n",
      "['nkm', 'soucie', 'image', 'deplorable', 'donnee', 'etranger', 'pays', 'paris', 'cause']\n",
      "['economiedelamer', 'jlm', 'franceinsoumise', 'avenirencommun', 'melenchon', 'tele']\n",
      "['macron', 'prend', 'pieds', 'tapis', 'triangulation']\n",
      "['macronpiegeacons', 'bayrou', 'continue', 'compagnonnage', 'allies', 'hollande', 'macron', 'hasbeen']\n",
      "['devaluation', 'selon', 'melenchon', 'pen', 'penalisera', 'salaries', 'retraites', 'contrepoints']\n",
      "['reste', 'plus', 'convaincre', 'tous', 'autres', 'ps', 'gouvernement']\n",
      "['eur', 'usd', 'francois', 'bayrou', 'has', 'reduced', 'political', 'risks', 'in', 'europe', 'ero']\n",
      "['excellent']\n",
      "['comment', 'faire', 'excuses', 'vraiment', 'techniques', 'utilisees', 'politiques']\n",
      "['lepen', 'presidente', 'peine', 'mort', 'tout', 'allez', 'rien', 'comprendre', 'gros', 'retour', 'arriere']\n",
      "['nicolas', 'dupont', 'aignan', 'anne', 'meaux', 'conseillere', 'fillon', 'aussi', 'conseillere', 'pdg', 'tf', 'bvoltaire']\n",
      "['reconnais', 'emmanuel', 'macron', 'incarne', 'pourquoi', 'unis', 'petit', 'probleme']\n",
      "['personne', 'attaquee', 'parce', 'heurte', 'consensus', 'mou']\n",
      "['quand', 'meme', 'negligeable', 'suffrages', 'plus']\n",
      "['communique', 'francois', 'hollande', 'affaire', 'fillon']\n",
      "['reaction', 'marine', 'lepen', 'apres', 'avoir', 'refuse', 'porter', 'voile', 'bravo', 'liban', 'beyrouth']\n",
      "['dirais', 'meme', 'histoire', 'begaie', 'oups', 'fait', 'vanne', 'bayrou']\n"
     ]
    }
   ],
   "source": [
    "for t in corpus['text'][41:60]:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3503, 3113)\n",
      "accuracy of the sklearn naive bayes :  0.346087949743\n"
     ]
    }
   ],
   "source": [
    "getSentiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
