{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pymongo as pym\n",
    "import nltk.data\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import stop_words\n",
    "from nltk.stem import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'je', 'la', 'le', 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'été', 'étée', 'étées', 'étés', 'étant', 'étante', 'étants', 'étantes', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent']\n",
      "---\n",
      "['a', 'ai', 'aie', 'aient', 'aies', 'ait', 'alors', 'as', 'au', 'aucun', 'aura', 'aurai', 'auraient', 'aurais', 'aurait', 'auras', 'aurez', 'auriez', 'aurions', 'aurons', 'auront', 'aussi', 'autre', 'aux', 'avaient', 'avais', 'avait', 'avant', 'avec', 'avez', 'aviez', 'avions', 'avoir', 'avons', 'ayant', 'ayez', 'ayons', 'bon', 'car', 'ce', 'ceci', 'cela', 'ces', 'cet', 'cette', 'ceux', 'chaque', 'ci', 'comme', 'comment', 'd', 'dans', 'de', 'dedans', 'dehors', 'depuis', 'des', 'deux', 'devoir', 'devrait', 'devrez', 'devriez', 'devrions', 'devrons', 'devront', 'dois', 'doit', 'donc', 'dos', 'droite', 'du', 'dès', 'début', 'dù', 'elle', 'elles', 'en', 'encore', 'es', 'est', 'et', 'eu', 'eue', 'eues', 'eurent', 'eus', 'eusse', 'eussent', 'eusses', 'eussiez', 'eussions', 'eut', 'eux', 'eûmes', 'eût', 'eûtes', 'faire', 'fais', 'faisez', 'fait', 'faites', 'fois', 'font', 'force', 'furent', 'fus', 'fusse', 'fussent', 'fusses', 'fussiez', 'fussions', 'fut', 'fûmes', 'fût', 'fûtes', 'haut', 'hors', 'ici', 'il', 'ils', 'j', 'je', 'juste', 'l', 'la', 'le', 'les', 'leur', 'leurs', 'lui', 'là', 'm', 'ma', 'maintenant', 'mais', 'me', 'mes', 'moi', 'moins', 'mon', 'mot', 'même', 'n', 'ne', 'ni', 'nom', 'nommé', 'nommée', 'nommés', 'nos', 'notre', 'nous', 'nouveau', 'nouveaux', 'on', 'ont', 'ou', 'où', 'par', 'parce', 'parole', 'pas', 'personne', 'personnes', 'peu', 'peut', 'plupart', 'pour', 'pourquoi', 'qu', 'quand', 'que', 'quel', 'quelle', 'quelles', 'quels', 'qui', 'sa', 'sans', 'se', 'sera', 'serai', 'seraient', 'serais', 'serait', 'seras', 'serez', 'seriez', 'serions', 'serons', 'seront', 'ses', 'seulement', 'si', 'sien', 'soi', 'soient', 'sois', 'soit', 'sommes', 'son', 'sont', 'sous', 'soyez', 'soyons', 'suis', 'sujet', 'sur', 't', 'ta', 'tandis', 'te', 'tellement', 'tels', 'tes', 'toi', 'ton', 'tous', 'tout', 'trop', 'très', 'tu', 'un', 'une', 'valeur', 'voient', 'vois', 'voit', 'vont', 'vos', 'votre', 'vous', 'vu', 'y', 'à', 'ça', 'étaient', 'étais', 'était', 'étant', 'état', 'étiez', 'étions', 'été', 'étés', 'êtes', 'être']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('french'))\n",
    "print('---')\n",
    "print(stop_words.get_stop_words('fr'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'‘', 'mm', 'vs', 'gds', 'si', 'b', 'qd', 'q', 'r', '``', 'e', 'gd', 'k', 'm', 'ن', 's', 'd', 'u', 'qu', '&', 'g', 'rt', 'ca', 'gt', ';', '@', 'ss', 'h', 'ac', 'nn', 'ns', 'ni', 'p', 'ds', '%', '$', 'l', 'pr', 'w', 'tt', 'le', 'via', 'a', 'c', 'ils', 'i', 'y', 'o', 'z', 'n', 'j', '£', ':', ' ', 'f', '^', 'x', 'amp', 'v', '€', 'les', 't'}\n"
     ]
    }
   ],
   "source": [
    "stops = set(['rt','ds','qd','ss','ns','vs','nn','amp','gt','gd','gds','tt','pr','ac','mm', 'qu',\n",
    "            '``', 'ni', 'ca', 'le', 'les', ' ', 'si', '$', '^', 'via', 'ils'] +\n",
    "            list('@ن%£€‘:&;') + list('abcdefghijklmnopqrstuvwxyz'))\n",
    "print(stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tweetPreprocessing(retweet=False):\n",
    "    client = pym.MongoClient('localhost', 27017)\n",
    "    #collection = client.tweet.cleaned\n",
    "    collection = client.tweet.train\n",
    "\n",
    "    # attention : dans la base \"labelised\" le champ s'appelle t_text et pas text\n",
    "    #collection2 = client.tweet.labelised\n",
    "\n",
    "    tweets = collection.find(filter={'text':{'$exists':True}}, projection={'_id':False})\n",
    "    #tweets2 = collection2.find(filter={'t_text':{'$exists':True}}, projection={'_id':False})\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    listTweets, listCandidats, listSentiments = [], [], []\n",
    "\n",
    "    for t in tweets: \n",
    "        if not retweet: # filtrage des retweets\n",
    "            if 'rt @' in t['text']:\n",
    "                continue\n",
    "        \n",
    "        # comptes\n",
    "        a = t['text'].count('!')\n",
    "        b = t['text'].count('?')\n",
    "        c = t['text'].count('#')\n",
    "        d = t['text'].count('\"')\n",
    "        e = t['text'].count('http')\n",
    "#         if e > 1:\n",
    "#             e = 1\n",
    "        \n",
    "        # mot tronqué\n",
    "        t['text'] = re.sub(r'\\w*…', '', t['text'])\n",
    "        \n",
    "        # caracteres speciaux, url et rt\n",
    "        t['text'] = re.sub(r'\\xad', '-',\n",
    "                           re.sub(r'\\n', ' ',\n",
    "                                  re.sub(r'\\W*(?!\\S)', '',\n",
    "                                         re.sub(r'(?:htt)\\S*', '',\n",
    "                                                re.sub(r'^rt.*: ', '',\n",
    "                                                       re.sub(r'\\d', '',\n",
    "                                                              re.sub(r',;:!?\\.\\/\\*(){}', '',\n",
    "                                                                     re.sub(r'«»', '', t['text']))))))))\n",
    "        \n",
    "        t['text'] = re.sub('|'.join(['’', '_', '-', '\\'', '\\.', '/', '“', '  ']), ' ', t['text'])\n",
    "        \n",
    "        # accents\n",
    "#         t['text'] = re.sub('|'.join('Ééèêë'), 'e', t['text'])\n",
    "#         t['text'] = re.sub('|'.join('àâä'), 'a', t['text'])\n",
    "#         t['text'] = re.sub('|'.join('ç'), 'c', t['text'])\n",
    "#         t['text'] = re.sub('|'.join('œ'), 'oe', t['text'])\n",
    "#         t['text'] = re.sub('|'.join('Ôôö'), 'o', t['text'])\n",
    "#         t['text'] = re.sub('|'.join('îï'), 'i', t['text'])\n",
    "#         t['text'] = re.sub('|'.join('ùû'), 'u', t['text'])\n",
    "        \n",
    "        # apostrophes\n",
    "        t['text'] = re.sub('|'.join([elem + '\\'' for elem in 'cdjlmnst']), '', t['text'])\n",
    "        \n",
    "        tokenizer = TreebankWordTokenizer()\n",
    "        t['text'] = tokenizer.tokenize(t['text'])\n",
    "        t['text'] = [token for token in t['text'] if (token not in stops) and (len(token)>2)]\n",
    "\n",
    "        while '' in t['text']:\n",
    "            t['text'].pop('')\n",
    "            \n",
    "        if t['text']: # test si liste non vide\n",
    "            #listTweets.append(list(set(t['text']))) # mots uniques\n",
    "            listTweets.append(t['text'])\n",
    "            try:\n",
    "                listCandidats.append(t['candidat'])\n",
    "            except:\n",
    "                listCandidats.append(None)\n",
    "\n",
    "            try:\n",
    "                listSentiments.append(t['sentiment'])\n",
    "            except:\n",
    "                listSentiments.append(None)\n",
    "                \n",
    "            rec = pd.DataFrame([[a, b, c, d, e]], columns=['!', '?', '#', '\"', '_http_'])\n",
    "            df = df.append(rec, ignore_index=True)\n",
    "        \n",
    "    df['text'], df['candidat'], df['sentiment'] = listTweets, listCandidats, listSentiments\n",
    "    \n",
    "    return df\n",
    "\n",
    "def build_feat_mat(df_tweets):\n",
    "    vectorizer = TfidfVectorizer(strip_accents='unicode', analyzer='word', decode_error='strict',\n",
    "                                use_idf=True, norm='l2', binary=False, min_df=.0005, max_df=1.)\n",
    "    X = vectorizer.fit_transform(df_tweets['text'].apply(' '.join))\n",
    "    hstack((X, df_tweets[['!', '?', '#', '\"', '_http_']]))\n",
    "\n",
    "    return X\n",
    "\n",
    "def getSentiments(n_predict, retweet) : \n",
    "    df = tweetPreprocessing(retweet)\n",
    "    df = df.sample(frac=1.0, replace=False) # mélange des lignes\n",
    "    \n",
    "    X = build_feat_mat(df)\n",
    "    y = df['sentiment']\n",
    "    \n",
    "    n_samples, vocabulaire = X.shape\n",
    "    print('Tweets : ' + str(n_samples) + ' / ' + 'Mots : ' + str(vocabulaire))\n",
    "    \n",
    "    #model = MultinomialNB()\n",
    "    #model = RandomForestClassifier(n_estimators=30, criterion='gini', max_depth=None, n_jobs=-1)\n",
    "    #model = KNeighborsClassifier(n_neighbors=15, weights='distance', algorithm='auto', leaf_size=30, p=2, metric='minkowski', n_jobs=-1)\n",
    "    model = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, class_weight='balanced')\n",
    "    \n",
    "    model.fit(X[:-n_predict], y[:-n_predict])\n",
    "    predictions = model.predict(X[n_samples - n_predict:])\n",
    "    \n",
    "    print('Score', np.sum(predictions == y[n_samples - n_predict:]) / len(predictions))\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille base d'entrainement : 3728\n",
      "-1.0    2380\n",
      " 0.0     950\n",
      " 1.0     398\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "corpus = tweetPreprocessing(retweet=True)\n",
    "print('Taille base d\\'entrainement :', corpus.shape[0])\n",
    "print(corpus['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>!</th>\n",
       "      <th>?</th>\n",
       "      <th>#</th>\n",
       "      <th>\"</th>\n",
       "      <th>_http_</th>\n",
       "      <th>text</th>\n",
       "      <th>candidat</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[des, français, veulent, interdire, aux, parle...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[pauvre, fillon, finir, par, devenir, informat...</td>\n",
       "      <td>None</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[pas, révolution, simplement, vielles, recette...</td>\n",
       "      <td>None</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[cette, presse, étrangère, est, aux, mains, de...</td>\n",
       "      <td>None</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[est, comme, avait, pas, pilote, dans, avion]</td>\n",
       "      <td>None</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[poireau, très, présent, mais, suis, très, trè...</td>\n",
       "      <td>None</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[deux, lois, benoît, hamon, bercy, ont, elles,...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[bayrou, tweetait, derriere, macron, des, inte...</td>\n",
       "      <td>None</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[gifles, valls, frôles, chaise, électrique, vi...</td>\n",
       "      <td>None</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[macron, presidentielle, ‼️il, faut, voter, fi...</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   !  ?  #  \"  _http_                                               text  \\\n",
       "0  0  0  0  0       1  [des, français, veulent, interdire, aux, parle...   \n",
       "1  0  0  0  0       1  [pauvre, fillon, finir, par, devenir, informat...   \n",
       "2  0  0  1  0       0  [pas, révolution, simplement, vielles, recette...   \n",
       "3  0  0  0  0       0  [cette, presse, étrangère, est, aux, mains, de...   \n",
       "4  0  0  0  2       1      [est, comme, avait, pas, pilote, dans, avion]   \n",
       "5  0  0  0  0       0  [poireau, très, présent, mais, suis, très, trè...   \n",
       "6  0  1  0  0       2  [deux, lois, benoît, hamon, bercy, ont, elles,...   \n",
       "7  0  0  0  1       0  [bayrou, tweetait, derriere, macron, des, inte...   \n",
       "8  0  0  0  2       0  [gifles, valls, frôles, chaise, électrique, vi...   \n",
       "9  0  0  3  0       1  [macron, presidentielle, ‼️il, faut, voter, fi...   \n",
       "\n",
       "  candidat  sentiment  \n",
       "0     None        0.0  \n",
       "1     None       -1.0  \n",
       "2     None       -1.0  \n",
       "3     None       -1.0  \n",
       "4     None       -1.0  \n",
       "5     None       -1.0  \n",
       "6     None        0.0  \n",
       "7     None       -1.0  \n",
       "8     None       -1.0  \n",
       "9     None        1.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets : 3728 / Mots : 3360\n",
      "Score 0.675\n",
      "5 37\n"
     ]
    }
   ],
   "source": [
    "a = getSentiments(200, retweet=True)\n",
    "print(len(a[a==1]), len(a[a==0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculer des f-score, sentiwordnet, bigrammes..."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
