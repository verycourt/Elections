{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [IMPORTANT] remettre les hashtags (ne pas appliquer de regex dessus)\n",
    "* rajouter des stop words\n",
    "* comment mixer les features tf-idf et les autres, pour leur donner du poids? Lire des articles,\n",
    "* faire deux modèles et mixer\n",
    "* comment gérer l'ironie?\n",
    "* Faire une fonction qui va uniquement prédire sur X derniers tweets de la collection tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pymongo as pym\n",
    "import nltk.data\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import stop_words\n",
    "from nltk.stem import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stops = set(['rt','ds','qd','ss','ns','vs','nn','amp','gt','gd','gds','tt','pr','ac','mm', 'qu',\n",
    "            '``', 'ni', 'ca', 'le', 'les', ' ', 'si', '$', '^', 'via', 'ils'] +\n",
    "            list('@ن%£€‘:&;') + list('abcdefghijklmnopqrstuvwxyz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def tweetPreprocessing(collection, nLastTweets=0, retweet=False):\n",
    "\n",
    "    tweets = collection.find(filter={'text':{'$exists':True}}, \n",
    "                             projection={'_id':False}).sort('$natural',-1).limit(nLastTweets)  \n",
    "    df = pd.DataFrame()\n",
    "    listTweets, listCandidats, listSentiments = [], [], []\n",
    "\n",
    "    for t in tweets: \n",
    "        if not retweet: # filtrage des retweets\n",
    "            if 'rt @' in t['text']:\n",
    "                continue\n",
    "        # comptes\n",
    "        a = t['text'].count('!')\n",
    "        b = t['text'].count('?')\n",
    "        c = t['text'].count('#')\n",
    "        d = t['text'].count('\"')\n",
    "        e = t['text'].count('http')\n",
    "        \n",
    "        # mot tronqué\n",
    "        t['text'] = re.sub(r'\\w*…', '', t['text'])\n",
    "        \n",
    "        # caracteres speciaux\n",
    "        t['text'] = re.sub(r'\\xad', '-',\n",
    "                           re.sub(r'\\n', ' ', # retrait des sauts de ligne\n",
    "                                  re.sub(r'\\W*(?!\\S)', '', # retrait de ce qui n'est pas un mot\n",
    "                                         re.sub(r'(?:htt)\\S*', '', # retrait des liens http\n",
    "                                                re.sub(r'^rt.*: ', '', # retrait de la mention retweet\n",
    "                                                       re.sub(r'\\d', '', # retrait des chiffres\n",
    "                                                              re.sub(r',;!?\\/\\*(){}«»', ' ', t['text'])))))))\n",
    "        \n",
    "        t['text'] = re.sub('|'.join(['’', '_', '/', '-', '\\'', '“', '\\.']), ' ', t['text'])\n",
    "        \n",
    "        # accents (il faut laisser ce bloc)\n",
    "#         t['text'] = re.sub('|'.join('Ééèêë'), 'e', t['text'])\n",
    "#         t['text'] = re.sub('|'.join('àâä'), 'a', t['text'])\n",
    "#         t['text'] = re.sub('|'.join('ç'), 'c', t['text'])\n",
    "#         t['text'] = re.sub('|'.join('œ'), 'oe', t['text'])\n",
    "#         t['text'] = re.sub('|'.join('Ôôö'), 'o', t['text'])\n",
    "#         t['text'] = re.sub('|'.join('îï'), 'i', t['text'])\n",
    "#         t['text'] = re.sub('|'.join('ùû'), 'u', t['text'])\n",
    "        \n",
    "        # apostrophes\n",
    "        t['text'] = re.sub('|'.join([elem + '\\'' for elem in 'cdjlmnst']), '', t['text'])\n",
    "        \n",
    "        tokenizer = TreebankWordTokenizer()\n",
    "        t['text'] = tokenizer.tokenize(t['text'])\n",
    "        t['text'] = [token for token in t['text'] if (token not in stops) and (len(token)>2)]\n",
    "\n",
    "        while '' in t['text']:\n",
    "            t['text'].pop('')\n",
    "            \n",
    "        if t['text']: # test si liste non vide\n",
    "            listTweets.append(t['text'])\n",
    "            try:\n",
    "                listCandidats.append(t['candidat'])\n",
    "            except:\n",
    "                listCandidats.append(None)\n",
    "\n",
    "            try:\n",
    "                listSentiments.append(t['sentiment'])\n",
    "            except:\n",
    "                listSentiments.append(None)\n",
    "                \n",
    "            rec = pd.DataFrame([[a, b, c, d, e]], columns=['!', '?', '#', '\"', '_http_'])\n",
    "            df = df.append(rec, ignore_index=True)\n",
    "        \n",
    "    df['text'], df['candidat'], df['sentiment'] = listTweets, listCandidats, listSentiments\n",
    "    return df\n",
    "\n",
    "def build_feat_mat(df_tweets):\n",
    "    vectorizer = TfidfVectorizer(strip_accents='unicode', analyzer='word', decode_error='strict',\n",
    "                                use_idf=True, norm='l2', binary=False, min_df=.0005, max_df=1.)\n",
    "    X = vectorizer.fit_transform(df_tweets['text'].apply(' '.join))\n",
    "    hstack((X, df_tweets[['!', '?', '#', '\"', '_http_']]))\n",
    "\n",
    "    return X\n",
    "\n",
    "def getSentiments(client, n_predict, retweet, full_retrain=True) : \n",
    "    if full_retrain :\n",
    "        # bases utilisées\n",
    "        df = tweetPreprocessing(client.tweet.train, 0, retweet)\n",
    "        df2 = tweetPreprocessing(client.tweet.labelised, 0, retweet)\n",
    "        try:\n",
    "            df = pd.concat([df, df2], axis=0, ignore_index=True)\n",
    "        except:\n",
    "            pass\n",
    "        df = df.sample(frac=1.0, replace=False) # mélange des lignes\n",
    "        X = build_feat_mat(df)\n",
    "        y = df['sentiment']\n",
    "        n_samples, vocabulaire = X.shape\n",
    "        print('Tweets : ' + str(n_samples) + ' / ' + 'Mots : ' + str(vocabulaire))\n",
    "        params = {'penalty':['l2'], 'C' : np.logspace(-1,1,100),\n",
    "                  'class_weight':[{-1.0:0.80, 0.0:0.05, 1.0:0.25},'balanced'], \n",
    "                  'solver':['lbfgs'], 'multi_class':['ovr','multinomial']}\n",
    "        lr = LogisticRegression()\n",
    "        GCV = GridSearchCV(lr, params, verbose=1, n_jobs=-1)\n",
    "        GCV.fit(X[:-n_predict], y[:-n_predict])\n",
    "        print(GCV.best_params_)\n",
    "        model = LogisticRegression(penalty=GCV.best_params_['penalty'],\n",
    "                                   C=GCV.best_params_['C'], \n",
    "                                   class_weight=GCV.best_params_['class_weight'],\n",
    "                                   solver=GCV.best_params_['solver'], multi_class=GCV.best_params_['multi_class'])   \n",
    "    \n",
    "    else :\n",
    "        f = open('sentiment_model.pkl','rb')\n",
    "        model = pickle.load(f)\n",
    "        f.close()\n",
    "        # bases utilisées\n",
    "        df = tweetPreprocessing(client.tweet.train, 500, retweet)\n",
    "        df2 = tweetPreprocessing(client.tweet.labelised, 500, retweet)\n",
    "        try:\n",
    "            df = pd.concat([df, df2], axis=0, ignore_index=True)\n",
    "        except:\n",
    "            pass\n",
    "        df = df.sample(frac=1.0, replace=False) # mélange des lignes\n",
    "        X = build_feat_mat(df)\n",
    "        y = df['sentiment']\n",
    "        n_samples, vocabulaire = X.shape\n",
    "        print('Tweets : ' + str(n_samples) + ' / ' + 'Mots : ' + str(vocabulaire))\n",
    "\n",
    "    model.fit(X[:-n_predict], y[:-n_predict])\n",
    "    predictions = model.predict(X[n_samples - n_predict:])\n",
    "    print('Score', np.sum(predictions == y[n_samples - n_predict:]) / len(predictions)) \n",
    "    f = open('sentiment_model.pkl','wb')\n",
    "    pickle.dump(model, f)\n",
    "    f.close()\n",
    "    print('Model saved')\n",
    "    return predictions\n",
    "\n",
    "def cleanTrainDb(collection):\n",
    "    textCleanPipeline = [{\"$group\":{\"_id\":\"$text\", \"dups\":{\"$push\":\"$_id\"},\"count\":{\"$sum\":1}}},{\"$match\":{\"count\":{\"$gt\":1}}}]\n",
    "    duplicates = []\n",
    "    count = 0\n",
    "    try:\n",
    "        for doc in collection.aggregate(textCleanPipeline) :\n",
    "            it = iter(doc['dups'])\n",
    "            next(it)\n",
    "            for id in it:\n",
    "                count += 1\n",
    "                duplicates.append(pym.DeleteOne({'_id':id}))\n",
    "        if duplicates:\n",
    "            collection.bulk_write(duplicates)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print(count, 'doublons retirés.')\n",
    "    client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4424\n",
      "1421\n"
     ]
    }
   ],
   "source": [
    "#client = pym.MongoClient('localhost', 27018)\n",
    "client = pym.MongoClient('localhost', 27017)\n",
    "collection = client.tweet.train\n",
    "collection2 = client.tweet.labelised\n",
    "print(collection.count())\n",
    "print(collection2.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test du nettoyage des tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = tweetPreprocessing(collection, retweet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['villumsen', 'jfchauffete', 'macron', 'refuse', 'emission', 'politique', 'tour', 'refuserait', 'serait', 'élu', 'sans', 'jamais', 'avoir', 'été', 'contredit']\n",
      "['rÉaction', 'bernard', 'lugan', 'aux', 'propos', 'tenus', 'par', 'macron']\n",
      "['appel', 'rue', 'fillon', 'rêve', 'gaulle', 'mais', 'chienlit', 'est', 'lui', 'lobs']\n",
      "['lui', 'entre', 'mariage', 'pour', 'tous', 'fillon', 'soupçonne', 'une', 'légère', 'schizophrénie', 'dans', 'son', 'engagement', 'chez']\n",
      "['mes', 'pensées', 'vont', 'faire', 'ces', 'choses', 'appelle', 'tolerance', 'instruction', 'rip']\n",
      "['mais', 'pas', 'pareil', 'est', 'gauche', 'donc', 'parfaitement', 'licite']\n",
      "['ydekerdrel', 'oui', 'mais', 'votre', 'soutien', 'fillon', 'démontre', 'que', 'vos', 'valeurs', 'sont', 'néant', 'assez', 'fourberies']\n",
      "['brunolemaire', 'droite', 'perdra', 'aussi', 'avec', 'fillon', 'perdra', 'son', 'honneur', 'devra', 'encore', 'trouver', 'nouveau', 'nom']\n",
      "['fillon', 'trocadéro', 'personnes', 'quand', 'bruno', 'retailleau', 'emballe']\n",
      "['ump', 'traîne', 'fillon', 'dans', 'pas', 'copé', 'canarddukiosque']\n",
      "['mathieugallard', 'des', 'sondages', 'macron', 'fillon', 'deuxième', 'tour', 'est', 'que', 'été', 'fait']\n",
      "['enmarche', 'pour', 'fossiles', 'vieux', 'monde', 'politique', 'bayrou', 'bourlanges', 'kouchner', 'minc', 'attali', 'royal']\n",
      "['macron', 'devrait', 'faire', 'campagne', 'pour', 'remplacer', 'bouteflika']\n",
      "['fillon', 'irai', 'jusqu', 'bout', 'voulait', 'sans', 'doute', 'dire', 'aller', 'nous', 'mettre', 'jusqu', 'bout']\n",
      "['marion', 'maréchal', 'pen', 'vous', 'continuez', 'fermer', 'yeux', 'vos', 'enfants', 'paieront', 'prix', 'bvoltaire']\n",
      "['fillon', 'francoisfillon', 'comme', 'impression', 'que', 'socialos', 'qui', 'soutiennent', 'macron', 'sont', 'inquiets']\n",
      "['macron', 'exposé', 'pourquoi', 'doit', 'parler', 'des', 'politiques', 'macroéconomiques', 'fait', 'parce', 'que', 'est', 'notre']\n",
      "['pourtant', 'gauche', 'est', 'spécialistes', 'mur', 'des', 'cons', 'fait', 'avaient', 'oublié', 'lui']\n",
      "['bfmtv', 'video', 'des', 'manifestants', 'anti', 'fillon', 'évacués', 'tourcoing']\n",
      "['dereymondmichel', 'boblecentriste', 'faisons', 'pas', 'amalgames', 'rapide', 'dans', 'eme', 'tour', 'fillon', 'pen', 'pourrait', 'regretter']\n"
     ]
    }
   ],
   "source": [
    "for a in a['text'][:20]:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test du nettoyage doublons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 doublons retirés.\n",
      "0 doublons retirés.\n"
     ]
    }
   ],
   "source": [
    "cleanTrainDb(collection)\n",
    "cleanTrainDb(collection2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train et test du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets : 5442 / Mots : 2718\n",
      "Fitting 3 folds for each of 400 candidates, totalling 1200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=-1)]: Done 716 tasks      | elapsed:   37.7s\n",
      "[Parallel(n_jobs=-1)]: Done 966 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1200 out of 1200 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'penalty': 'l2', 'solver': 'lbfgs', 'class_weight': 'balanced', 'C': 0.44306214575838809, 'multi_class': 'ovr'}\n",
      "Score 0.627\n",
      "Model saved\n",
      "330 242\n"
     ]
    }
   ],
   "source": [
    "a = getSentiments(client, 1000, retweet=True, full_retrain=True)\n",
    "# attention en cas de retrain périodique on ne prend que les 500 derniers tweets, il faut donc prédire sur moins de 500\n",
    "print(len(a[a==1]), len(a[a==0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# calculer des f-score, sentiwordnet, bigrammes..."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
