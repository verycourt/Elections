{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pymongo as pym\n",
    "import nltk.data\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import stop_words\n",
    "from nltk.stem import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'je', 'la', 'le', 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'été', 'étée', 'étées', 'étés', 'étant', 'étante', 'étants', 'étantes', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent']\n",
      "---\n",
      "['a', 'ai', 'aie', 'aient', 'aies', 'ait', 'alors', 'as', 'au', 'aucun', 'aura', 'aurai', 'auraient', 'aurais', 'aurait', 'auras', 'aurez', 'auriez', 'aurions', 'aurons', 'auront', 'aussi', 'autre', 'aux', 'avaient', 'avais', 'avait', 'avant', 'avec', 'avez', 'aviez', 'avions', 'avoir', 'avons', 'ayant', 'ayez', 'ayons', 'bon', 'car', 'ce', 'ceci', 'cela', 'ces', 'cet', 'cette', 'ceux', 'chaque', 'ci', 'comme', 'comment', 'd', 'dans', 'de', 'dedans', 'dehors', 'depuis', 'des', 'deux', 'devoir', 'devrait', 'devrez', 'devriez', 'devrions', 'devrons', 'devront', 'dois', 'doit', 'donc', 'dos', 'droite', 'du', 'dès', 'début', 'dù', 'elle', 'elles', 'en', 'encore', 'es', 'est', 'et', 'eu', 'eue', 'eues', 'eurent', 'eus', 'eusse', 'eussent', 'eusses', 'eussiez', 'eussions', 'eut', 'eux', 'eûmes', 'eût', 'eûtes', 'faire', 'fais', 'faisez', 'fait', 'faites', 'fois', 'font', 'force', 'furent', 'fus', 'fusse', 'fussent', 'fusses', 'fussiez', 'fussions', 'fut', 'fûmes', 'fût', 'fûtes', 'haut', 'hors', 'ici', 'il', 'ils', 'j', 'je', 'juste', 'l', 'la', 'le', 'les', 'leur', 'leurs', 'lui', 'là', 'm', 'ma', 'maintenant', 'mais', 'me', 'mes', 'moi', 'moins', 'mon', 'mot', 'même', 'n', 'ne', 'ni', 'nom', 'nommé', 'nommée', 'nommés', 'nos', 'notre', 'nous', 'nouveau', 'nouveaux', 'on', 'ont', 'ou', 'où', 'par', 'parce', 'parole', 'pas', 'personne', 'personnes', 'peu', 'peut', 'plupart', 'pour', 'pourquoi', 'qu', 'quand', 'que', 'quel', 'quelle', 'quelles', 'quels', 'qui', 'sa', 'sans', 'se', 'sera', 'serai', 'seraient', 'serais', 'serait', 'seras', 'serez', 'seriez', 'serions', 'serons', 'seront', 'ses', 'seulement', 'si', 'sien', 'soi', 'soient', 'sois', 'soit', 'sommes', 'son', 'sont', 'sous', 'soyez', 'soyons', 'suis', 'sujet', 'sur', 't', 'ta', 'tandis', 'te', 'tellement', 'tels', 'tes', 'toi', 'ton', 'tous', 'tout', 'trop', 'très', 'tu', 'un', 'une', 'valeur', 'voient', 'vois', 'voit', 'vont', 'vos', 'votre', 'vous', 'vu', 'y', 'à', 'ça', 'étaient', 'étais', 'était', 'étant', 'état', 'étiez', 'étions', 'été', 'étés', 'êtes', 'être']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('french'))\n",
    "print('---')\n",
    "print(stop_words.get_stop_words('fr'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'v', 'gt', 'b', 'r', 'tt', 'd', 'gds', 'f', 'n', 'y', 'c', ';', 'j', 'amp', 'k', 'p', 'ils', 'vs', 'rt', 'a', 'l', 'q', 'nn', 'e', ':', 'via', 'mm', 'w', 'pr', 'qd', '``', 'ca', 'h', 'g', 'ni', 'le', '&', 't', '‘', '^', 'i', 'u', 'ds', 'ns', '%', '£', 'o', ' ', 'x', 'les', '$', 's', 'ss', 'gd', 'ن', 'qu', 'si', 'm', '€', 'z', 'ac', '@'}\n"
     ]
    }
   ],
   "source": [
    "stops = set(['rt','ds','qd','ss','ns','vs','nn','amp','gt','gd','gds','tt','pr','ac','mm', 'qu',\n",
    "            '``', 'ni', 'ca', 'le', 'les', ' ', 'si', '$', '^', 'via', 'ils'] +\n",
    "            list('@ن%£€‘:&;') + list('abcdefghijklmnopqrstuvwxyz'))\n",
    "print(stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tweetPreprocessing(collection, retweet=False):\n",
    "\n",
    "    tweets = collection.find(filter={'text':{'$exists':True}}, projection={'_id':False})\n",
    "    #tweets2 = collection2.find(filter={'t_text':{'$exists':True}}, projection={'_id':False})\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    listTweets, listCandidats, listSentiments = [], [], []\n",
    "\n",
    "    for t in tweets: \n",
    "        if not retweet: # filtrage des retweets\n",
    "            if 'rt @' in t['text']:\n",
    "                continue\n",
    "        \n",
    "        # comptes\n",
    "        a = t['text'].count('!')\n",
    "        b = t['text'].count('?')\n",
    "        c = t['text'].count('#')\n",
    "        d = t['text'].count('\"')\n",
    "        e = t['text'].count('http')\n",
    "#         if e > 1:\n",
    "#             e = 1\n",
    "        \n",
    "        # mot tronqué\n",
    "        t['text'] = re.sub(r'\\w*…', '', t['text'])\n",
    "        \n",
    "        # caracteres speciaux, url et rt\n",
    "        t['text'] = re.sub(r'\\xad', '-',\n",
    "                           re.sub(r'\\n', ' ',\n",
    "                                  re.sub(r'\\W*(?!\\S)', '',\n",
    "                                         re.sub(r'(?:htt)\\S*', '',\n",
    "                                                re.sub(r'^rt.*: ', '',\n",
    "                                                       re.sub(r'\\d', '',\n",
    "                                                              re.sub(r',;:!?\\.\\/\\*(){}', '',\n",
    "                                                                     re.sub(r'«»', '', t['text']))))))))\n",
    "        \n",
    "        t['text'] = re.sub('|'.join(['’', '_', '-', '\\'', '\\.', '/', '“', '  ']), ' ', t['text'])\n",
    "        \n",
    "        # accents\n",
    "#         t['text'] = re.sub('|'.join('Ééèêë'), 'e', t['text'])\n",
    "#         t['text'] = re.sub('|'.join('àâä'), 'a', t['text'])\n",
    "#         t['text'] = re.sub('|'.join('ç'), 'c', t['text'])\n",
    "#         t['text'] = re.sub('|'.join('œ'), 'oe', t['text'])\n",
    "#         t['text'] = re.sub('|'.join('Ôôö'), 'o', t['text'])\n",
    "#         t['text'] = re.sub('|'.join('îï'), 'i', t['text'])\n",
    "#         t['text'] = re.sub('|'.join('ùû'), 'u', t['text'])\n",
    "        \n",
    "        # apostrophes\n",
    "        t['text'] = re.sub('|'.join([elem + '\\'' for elem in 'cdjlmnst']), '', t['text'])\n",
    "        \n",
    "        tokenizer = TreebankWordTokenizer()\n",
    "        t['text'] = tokenizer.tokenize(t['text'])\n",
    "        t['text'] = [token for token in t['text'] if (token not in stops) and (len(token)>2)]\n",
    "\n",
    "        while '' in t['text']:\n",
    "            t['text'].pop('')\n",
    "            \n",
    "        if t['text']: # test si liste non vide\n",
    "            #listTweets.append(list(set(t['text']))) # mots uniques\n",
    "            listTweets.append(t['text'])\n",
    "            try:\n",
    "                listCandidats.append(t['candidat'])\n",
    "            except:\n",
    "                listCandidats.append(None)\n",
    "\n",
    "            try:\n",
    "                listSentiments.append(t['sentiment'])\n",
    "            except:\n",
    "                listSentiments.append(None)\n",
    "                \n",
    "            rec = pd.DataFrame([[a, b, c, d, e]], columns=['!', '?', '#', '\"', '_http_'])\n",
    "            df = df.append(rec, ignore_index=True)\n",
    "        \n",
    "    df['text'], df['candidat'], df['sentiment'] = listTweets, listCandidats, listSentiments\n",
    "    \n",
    "    return df\n",
    "\n",
    "def build_feat_mat(df_tweets):\n",
    "    vectorizer = TfidfVectorizer(strip_accents='unicode', analyzer='word', decode_error='strict',\n",
    "                                use_idf=True, norm='l2', binary=False, min_df=.0005, max_df=1.)\n",
    "    X = vectorizer.fit_transform(df_tweets['text'].apply(' '.join))\n",
    "    hstack((X, df_tweets[['!', '?', '#', '\"', '_http_']]))\n",
    "\n",
    "    return X\n",
    "\n",
    "def getSentiments(client, n_predict, retweet) : \n",
    "    # bases utilisées\n",
    "    df = tweetPreprocessing(client.tweet.train, retweet)\n",
    "    #df2 = tweetPreprocessing(client.tweet.labelised, retweet)\n",
    "    \n",
    "    try:\n",
    "        df = pd.concat([df, df2], axis=0, ignore_index=True)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    df = df.sample(frac=1.0, replace=False) # mélange des lignes\n",
    "    \n",
    "    X = build_feat_mat(df)\n",
    "    y = df['sentiment']\n",
    "    \n",
    "    n_samples, vocabulaire = X.shape\n",
    "    print('Tweets : ' + str(n_samples) + ' / ' + 'Mots : ' + str(vocabulaire))\n",
    "    \n",
    "    #model = MultinomialNB()\n",
    "    #model = RandomForestClassifier(n_estimators=30, criterion='gini', max_depth=None, n_jobs=-1)\n",
    "    #model = KNeighborsClassifier(n_neighbors=15, weights='distance', algorithm='auto', leaf_size=30, p=2, metric='minkowski', n_jobs=-1)\n",
    "    model = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, class_weight='balanced')\n",
    "    \n",
    "    model.fit(X[:-n_predict], y[:-n_predict])\n",
    "    predictions = model.predict(X[n_samples - n_predict:])\n",
    "    \n",
    "    print('Score', np.sum(predictions == y[n_samples - n_predict:]) / len(predictions))\n",
    "    return predictions\n",
    "\n",
    "def cleanTrainDb(collection):\n",
    "    textCleanPipeline = [{\"$group\":{\"_id\":\"$text\", \"dups\":{\"$push\":\"$_id\"},\"count\":{\"$sum\":1}}},{\"$match\":{\"count\":{\"$gt\":1}}}]\n",
    "    duplicates = []\n",
    "    count = 0\n",
    "    \n",
    "    try:\n",
    "        for doc in collection.aggregate(textCleanPipeline) :\n",
    "            it = iter(doc['dups'])\n",
    "            next(it)\n",
    "            for id in it:\n",
    "                count += 1\n",
    "                duplicates.append(pym.DeleteOne({'_id':id}))\n",
    "        if duplicates:\n",
    "            collection.bulk_write(duplicates)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print(count, 'doublons retirés.')\n",
    "    client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4044"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = pym.MongoClient('localhost', 27017)\n",
    "collection = client.tweet.train\n",
    "# collection = client.tweet.labelised\n",
    "collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 doublons retirés.\n"
     ]
    }
   ],
   "source": [
    "cleanTrainDb(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = tweetPreprocessing(collection, retweet=True)\n",
    "print('Taille base d\\'entrainement :', corpus.shape[0])\n",
    "print(corpus['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['présidentielle', 'fillon', 'double', 'macron', 'selon', 'sondage', 'pour', 'express', 'bfmtv', 'fillon', 'fillon', 'macron']\n",
      "['récents', 'événements', 'ont', 'confirmé', 'que', 'fillon', 'avait', 'des', 'nerfs', 'acier', 'rassurant', 'pour', 'futur', 'président', 'fillon']\n",
      "['sondage', 'elabe', 'pour', 'bfmtv', 'fillon', 'remonte', 'macron', 'effondre', 'fillon', 'macrongirouette', 'gauche']\n",
      "['allocations', 'familiales', 'aux', 'familles', 'mineurs', 'récidivistes', 'français', 'réagissent', 'macron']\n",
      "['macron', 'girouette', 'anguille', 'fillon']\n",
      "['sondage', 'fillon', 'double', 'macron', 'premier', 'tour', 'avec', 'sans', 'bayrou', 'lexpress', 'fillonprésident', 'fillon']\n",
      "['points', 'pour', 'fillon', 'points', 'pour', 'macron', 'sondage', 'elabe', 'presidentielle']\n",
      "['aucune', 'crédibilité', 'fillon', 'présenter', 'présidentielle', 'est', 'honteux', 'part', 'élu', 'corrompu']\n",
      "['sulfureux', 'employeur', 'porte', 'parole', 'françois', 'fillon', 'fillon']\n",
      "['courbes', 'croisent', 'février', 'comme', 'chirac', 'balladur', 'francoisfillon', 'fillon', 'udi']\n",
      "['connais', 'qui', 'accepté', 'voiler', 'face', 'étranger', 'fillon', 'fillon', 'bonnetbleu', 'cdanslair']\n",
      "['allô', 'dernier', 'sondage', 'macron', 'fillon', 'hamon', 'lepen', 'melenchon']\n",
      "['fillon', 'fillon', 'repasse', 'devant', 'macron']\n",
      "['lepionmacron', 'roi', 'marketing', 'politique', 'fillon']\n",
      "['blues', 'mediapart', 'dès', 'que', 'fillon', 'remonte', 'plenel', 'rajoute', 'une', 'couche', 'vernis', 'est', 'pour', 'hamon']\n",
      "['mehdimeklat', 'phénomene', 'isolé', 'tendance', 'fillon', 'mlp', 'hamon', 'dupontaignan']\n",
      "['alors', 'plutôt', 'macron', 'lundi', 'macron', 'vendredi', 'toutsaufmacron', 'fillon']\n",
      "['sante', 'projet', 'amélioré', 'enrichi', 'fillon', 'projetfillon']\n",
      "['cochet', 'fillon', 'gars', 'profitent', 'bien', 'argent', 'public', 'beaux', 'donneurs', 'leçons', 'fillon']\n",
      "['fillon', 'soutien', 'juppe', 'est', 'non', 'seulement', 'bien', 'venu', 'mais', 'également', 'signal', 'fort', 'unité', 'bayrou', 'être', 'trop', 'triste', 'nous', 'ravi']\n"
     ]
    }
   ],
   "source": [
    "# remettre les # (ne pas appliquer de regex dessus)\n",
    "# rajouter des stop words\n",
    "# comment mixer les features tf-idf et les autres, pour leur donner du poids? Lire des articles,\n",
    "# faire deux modèles et mixer\n",
    "# comment gérer l'ironie?\n",
    "\n",
    "for i in range(20):\n",
    "    print(corpus['text'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train et test du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets : 1587 / Mots : 4808\n",
      "Score 0.513333333333\n",
      "17 99\n"
     ]
    }
   ],
   "source": [
    "a = getSentiments(client, 300, retweet=False)\n",
    "print(len(a[a==1]), len(a[a==0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculer des f-score, sentiwordnet, bigrammes..."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
