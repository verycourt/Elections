{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* comment mixer les features tf-idf et les autres, pour leur donner du poids? Lire des articles,\n",
    "* faire deux modèles et mixer\n",
    "* comment gérer l'ironie?\n",
    "* Faire une fonction qui va uniquement prédire sur X derniers tweets de la collection tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pymongo as pym\n",
    "import nltk.data\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import stop_words\n",
    "from nltk.stem import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stops = set(['rt','ds','qd','ss','ns','vs','nn','amp','gt','gd','gds','tt','pr','ac','mm', 'qu',\n",
    "            '``', 'ni', 'ca', 'le', 'les', ' ', 'si', '$', '^', 'via', 'ils','pour','une','que','quel'] +\n",
    "            list('@ن%£€‘:&;') + list('abcdefghijklmnopqrstuvwxyz')+\n",
    "            stop_words.get_stop_words(language='fr')+stopwords.words('french'))\n",
    "for e in ['ne','pas','sans'] :\n",
    "    stops.remove(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def tweetPreprocessing(collection, nLastTweets=0, retweet=False):\n",
    "\n",
    "    tweets = collection.find(filter={'text':{'$exists':True}}, \n",
    "                             projection={'_id':False}).sort('$natural',-1).limit(nLastTweets)  \n",
    "    df = pd.DataFrame()\n",
    "    listTweets, listCandidats, listSentiments = [], [], []\n",
    "\n",
    "    for t in tweets: \n",
    "        if not retweet: # filtrage des retweets\n",
    "            if 'rt @' in t['text']:\n",
    "                continue\n",
    "        # comptes\n",
    "        a = t['text'].count('!')\n",
    "        b = t['text'].count('?')\n",
    "        c = t['text'].count('#')\n",
    "        d = t['text'].count('\"')\n",
    "        e = t['text'].count('http')\n",
    "        f = t['text'].count(':')\n",
    "        g = t['text'].count('»')\n",
    "        h = t['text'].count('@')\n",
    "        \n",
    "        # mot tronqué\n",
    "        t['text'] = re.sub(r'\\w*…', '', t['text'])\n",
    "        \n",
    "        # caracteres speciaux\n",
    "        t['text'] = re.sub(r'\\xad', '-',\n",
    "                           re.sub(r'\\n', ' ', # retrait des sauts de ligne\n",
    "                       #           re.sub(r'\\[a-zA-Z]*(?!\\S)', '', # retrait de ce qui n'est pas un mot\n",
    "                                         re.sub(r'(?:htt)\\S*', '', # retrait des liens http\n",
    "                                                re.sub(r'^rt.*: ', '', # retrait de la mention retweet\n",
    "                                                       re.sub(r'\\d', '', # retrait des chiffres\n",
    "                                                              re.sub(r',;!?\\/\\*(){}«»', ' ', t['text']))))))\n",
    "        \n",
    "        t['text'] = re.sub('|'.join(['’', '_', '/', '-', '\\'', '“', '\\.']), ' ', t['text'])\n",
    "        \n",
    "        # accents (il faut laisser ce bloc)\n",
    "#         t['text'] = re.sub('|'.join('Ééèêë'), 'e', t['text'])\n",
    "#         t['text'] = re.sub('|'.join('àâä'), 'a', t['text'])\n",
    "#         t['text'] = re.sub('|'.join('ç'), 'c', t['text'])\n",
    "#         t['text'] = re.sub('|'.join('œ'), 'oe', t['text'])\n",
    "#         t['text'] = re.sub('|'.join('Ôôö'), 'o', t['text'])\n",
    "#         t['text'] = re.sub('|'.join('îï'), 'i', t['text'])\n",
    "#         t['text'] = re.sub('|'.join('ùû'), 'u', t['text'])\n",
    "        \n",
    "        # apostrophes\n",
    "        t['text'] = re.sub('|'.join([elem + '\\'' for elem in 'cdjlmnst']), '', t['text'])\n",
    "        \n",
    "        tokenizer = TreebankWordTokenizer()\n",
    "        t['text'] = tokenizer.tokenize(t['text'])\n",
    "        t['text'] = [token for token in t['text'] if (token not in stops) and (len(token)>2)]\n",
    "        negation = 0\n",
    "        for token in t['text'] : \n",
    "            if token in ['ne','pas','n\\'','n','sans'] : \n",
    "                negation = 1\n",
    "        while '' in t['text']:\n",
    "            t['text'].pop('')\n",
    "            \n",
    "        if t['text']: # test si liste non vide\n",
    "            listTweets.append(t['text'])\n",
    "            try:\n",
    "                listCandidats.append(t['candidat'])\n",
    "            except:\n",
    "                listCandidats.append(None)\n",
    "            \n",
    "            try:\n",
    "                listSentiments.append(t['sentiment'])\n",
    "            except:\n",
    "                listSentiments.append(None)\n",
    "                \n",
    "            rec = pd.DataFrame([[a, b, c, d, e, f,g, h, negation]], \n",
    "                               columns=['!', '?', '#', '\"', '_http_',':','»','@','négation'])\n",
    "            df = df.append(rec, ignore_index=True)\n",
    "        \n",
    "    df['text'], df['candidat'], df['sentiment'] = listTweets, listCandidats, listSentiments\n",
    "    return df\n",
    "\n",
    "def build_feat_mat(df_tweets):\n",
    "    vectorizer = TfidfVectorizer(strip_accents='unicode', analyzer='word', decode_error='strict',\n",
    "                                use_idf=True, norm='l1', binary=False, min_df=.0001, max_df=0.6)\n",
    "    X = vectorizer.fit_transform(df_tweets['text'].apply(' '.join))\n",
    "    hstack((X, df_tweets[['!', '?', '#', '\"', '_http_',':','»']]))\n",
    "\n",
    "    return X\n",
    "\n",
    "def getSentiments(client, n_predict, retweet, full_retrain=True) : \n",
    "    if full_retrain :\n",
    "        # bases utilisées\n",
    "        df = tweetPreprocessing(client.tweet.train, 0, retweet)\n",
    "        df2 = tweetPreprocessing(client.tweet.labelised, 0, retweet)\n",
    "        try:\n",
    "            df = pd.concat([df, df2], axis=0, ignore_index=True)\n",
    "        except:\n",
    "            pass\n",
    "        df = df.sample(frac=1.0, replace=False) # mélange des lignes\n",
    "        X = build_feat_mat(df)\n",
    "        y = df['sentiment']\n",
    "        n_samples, vocabulaire = X.shape\n",
    "        print('Tweets : ' + str(n_samples) + ' / ' + 'Mots : ' + str(vocabulaire))\n",
    "        params = {'penalty':['l2'], 'C' : np.logspace(-2,2,100), \n",
    "                  'solver':['lbfgs'], 'multi_class':['ovr','multinomial']}\n",
    "        lr = LogisticRegression(max_iter=500)\n",
    "        GCV = GridSearchCV(lr, params, verbose=1, n_jobs=-1)\n",
    "        GCV.fit(X[:-n_predict], y[:-n_predict])\n",
    "        print(GCV.best_params_)\n",
    "        model = LogisticRegression(penalty=GCV.best_params_['penalty'],\n",
    "                                   C=GCV.best_params_['C'], \n",
    "                                   solver=GCV.best_params_['solver'], multi_class=GCV.best_params_['multi_class'])   \n",
    "    \n",
    "    else :\n",
    "        f = open('sentiment_model.pkl','rb')\n",
    "        model = pickle.load(f)\n",
    "        f.close()\n",
    "        # bases utilisées\n",
    "        df = tweetPreprocessing(client.tweet.train, 500, retweet)\n",
    "        df2 = tweetPreprocessing(client.tweet.labelised, 500, retweet)\n",
    "        try:\n",
    "            df = pd.concat([df, df2], axis=0, ignore_index=True)\n",
    "        except:\n",
    "            pass\n",
    "        df = df.sample(frac=1.0, replace=False) # mélange des lignes\n",
    "        X = build_feat_mat(df)\n",
    "        y = df['sentiment']\n",
    "        n_samples, vocabulaire = X.shape\n",
    "        print('Tweets : ' + str(n_samples) + ' / ' + 'Mots : ' + str(vocabulaire))\n",
    "\n",
    "    model.fit(X[:-n_predict], y[:-n_predict])\n",
    "    predictions = model.predict(X[n_samples - n_predict:])\n",
    "    print('Score', np.sum(predictions == y[n_samples - n_predict:]) / len(predictions)) \n",
    "    f = open('sentiment_model.pkl','wb')\n",
    "    pickle.dump(model, f)\n",
    "    f.close()\n",
    "    print('Model saved')\n",
    "    return predictions\n",
    "\n",
    "def cleanTrainDb(collection):\n",
    "    textCleanPipeline = [{\"$group\":{\"_id\":\"$text\", \"dups\":{\"$push\":\"$_id\"},\"count\":{\"$sum\":1}}},{\"$match\":{\"count\":{\"$gt\":1}}}]\n",
    "    duplicates = []\n",
    "    count = 0\n",
    "    try:\n",
    "        for doc in collection.aggregate(textCleanPipeline) :\n",
    "            it = iter(doc['dups'])\n",
    "            next(it)\n",
    "            for id in it:\n",
    "                count += 1\n",
    "                duplicates.append(pym.DeleteOne({'_id':id}))\n",
    "        if duplicates:\n",
    "            collection.bulk_write(duplicates)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print(count, 'doublons retirés.')\n",
    "    client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6246\n",
      "1421\n"
     ]
    }
   ],
   "source": [
    "client = pym.MongoClient('localhost', 27018)\n",
    "#client = pym.MongoClient('localhost', 27017)\n",
    "collection = client.tweet.train\n",
    "collection2 = client.tweet.labelised\n",
    "print(collection.count())\n",
    "print(collection2.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Test du nettoyage des tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "a = tweetPreprocessing(collection, retweet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for a in a['text'][:20]:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Test du nettoyage doublons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 doublons retirés.\n",
      "0 doublons retirés.\n"
     ]
    }
   ],
   "source": [
    "cleanTrainDb(collection)\n",
    "cleanTrainDb(collection2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train et test du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets : 7635 / Mots : 12540\n",
      "Fitting 3 folds for each of 200 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  44 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=-1)]: Done 194 tasks      | elapsed:   23.9s\n",
      "[Parallel(n_jobs=-1)]: Done 444 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:  4.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 17.07352647470692, 'multi_class': 'ovr', 'solver': 'lbfgs', 'penalty': 'l2'}\n",
      "Score 0.636666666667\n",
      "Model saved\n",
      "135 114\n"
     ]
    }
   ],
   "source": [
    "a = getSentiments(client, 600, retweet=True, full_retrain=True)\n",
    "# attention en cas de retrain périodique on ne prend que les 500 derniers tweets, il faut donc prédire sur moins de 500\n",
    "print(len(a[a==1]), len(a[a==0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Essai sur un Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def MAPE(preds, dtrain):\n",
    "    \"\"\"\n",
    "    This is the function used by XGBoost as metric.\n",
    "    \"\"\"\n",
    "    labels = dtrain.get_label()\n",
    "    return \"MAPE\", np.mean(np.abs((labels - preds) / labels)) * 100\n",
    "\n",
    "def build_feat_mat(df_tweets):\n",
    "    vectorizer = TfidfVectorizer(strip_accents='unicode', analyzer='word', decode_error='strict',\n",
    "                                use_idf=True, norm='l2', binary=False, min_df=.0005, max_df=0.5)\n",
    "    tfidf = vectorizer.fit_transform(df_tweets['text'].apply(' '.join))\n",
    "    X = pd.DataFrame(tfidf.toarray())\n",
    "    X[['!', '?', '#', '\"', '_http_',':','»','@','négation']] = df_tweets[['!', '?', '#', '\"', '_http_',':','»','@','négation']]\n",
    "    return X\n",
    "\n",
    "\n",
    "def TrainXGBoostModel(client, retweet=True, gpuEnabled=False):\n",
    "    df = tweetPreprocessing(client.tweet.train, 0, retweet)\n",
    "    df2 = tweetPreprocessing(client.tweet.labelised, 0, retweet)\n",
    "    try:\n",
    "        df = pd.concat([df, df2], axis=0, ignore_index=True)\n",
    "    except:\n",
    "        pass\n",
    "    df = df.sample(frac=1.0, replace=False) # mélange des lignes\n",
    "    X = build_feat_mat(df)\n",
    "    df[df['sentiment'] == -1] = 2\n",
    "    y = df['sentiment']\n",
    "    x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.2) # 20 % for validation\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2) # 20 % for test\n",
    "    n_samples, vocabulaire = X.shape\n",
    "    print('Tweets : ' + str(n_samples) + ' / ' + 'Mots : ' + str(vocabulaire))\n",
    "\n",
    "    boostRounds = 10000\n",
    "    # XGBoost data structure\n",
    "    xgtrain = xgb.DMatrix(x_train.values, y_train.values)\n",
    "    xgeval = xgb.DMatrix(x_val.values, y_val.values)\n",
    "    xgtest = xgb.DMatrix(x_test.values)\n",
    "     \n",
    "    \n",
    "    # Generate the watchlist\n",
    "    watchlist = [(xgtrain, \"train\"), (xgeval, \"eval\")]\n",
    "    print(\"Train the XGBoost model...\")\n",
    "    startTime = time.time()\n",
    "    param_test1 = {'max_depth':range(3,10,1), 'min_child_weight':range(1,6,1)}\n",
    "    \n",
    "    gsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5, \n",
    "    min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "    objective= 'multi:softmax', nthread=3, scale_pos_weight=1, seed=27), \n",
    "    param_grid = param_test1, scoring='roc_auc',n_jobs=3,iid=False, cv=5, verbose=9)\n",
    "    \n",
    "    gsearch1.fit(x_train[:500], y_train[:500])\n",
    "    clf = xgb.train(param_test1, xgtrain, boostRounds, evals = watchlist, maximize = True, verbose_eval = 100)\n",
    "\n",
    "    #Make predictions\n",
    "    print(\"Make predictions for testing set...\")\n",
    "    predictions_sr = pd.Series(clf.predict(xgtest, ntree_limit = clf.best_iteration), index = x_test.index)\n",
    "    return predictions_sr, clf, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets : 7635 / Mots : 2568\n",
      "Train the XGBoost model...\n",
      "Fitting 5 folds for each of 35 candidates, totalling 175 fits\n",
      "[CV] min_child_weight=1, max_depth=3 .................................\n",
      "[CV] min_child_weight=1, max_depth=3 .................................\n",
      "[CV] min_child_weight=1, max_depth=3 .................................\n"
     ]
    }
   ],
   "source": [
    "preds, model, truth = TrainXGBoostModel(client)\n",
    "print('Score', np.sum(truth  == preds) / len(preds)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
