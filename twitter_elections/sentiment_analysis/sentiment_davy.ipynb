{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Gérer les adverbes\n",
    "* comment mixer les features tf-idf et les autres, pour leur donner du poids? Lire des articles,\n",
    "* faire deux modèles et mixer\n",
    "* comment gérer l'ironie?\n",
    "* Faire une fonction qui va uniquement prédire sur X derniers tweets de la collection tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pymongo as pym\n",
    "import nltk.data\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import stop_words\n",
    "from nltk.stem import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stops = set(['rt','ds','qd','ss','ns','vs','nn','amp','gt','gd','gds','tt','pr','ac','mm', 'qu',\n",
    "            '``', 'ni', 'ca', 'le', 'les', ' ', 'si', '$', '^', 'via', 'ils','pour','une','que','quel'] +\n",
    "            list('@ن%£€‘:&;') + list('abcdefghijklmnopqrstuvwxyz')+\n",
    "            stop_words.get_stop_words(language='fr')+stopwords.words('french'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def tweetPreprocessing(collection, nLastTweets=0, retweet=False):\n",
    "\n",
    "    tweets = collection.find(filter={'text':{'$exists':True}}, \n",
    "                             projection={'_id':False}).sort('$natural',-1).limit(nLastTweets)  \n",
    "    df = pd.DataFrame()\n",
    "    listTweets, listCandidats, listSentiments = [], [], []\n",
    "\n",
    "    for t in tweets: \n",
    "        if not retweet: # filtrage des retweets\n",
    "            if 'rt @' in t['text']:\n",
    "                continue\n",
    "        # comptes\n",
    "        a = t['text'].count('!')\n",
    "        b = t['text'].count('?')\n",
    "        c = t['text'].count('#')\n",
    "        d = t['text'].count('\"')\n",
    "        e = t['text'].count('http')\n",
    "        f = t['text'].count(':')\n",
    "        g = t['text'].count('»')\n",
    "        h = t['text'].count('@')\n",
    "        \n",
    "        # mot tronqué\n",
    "        t['text'] = re.sub(r'\\w*…', '', t['text'])\n",
    "        \n",
    "        # caracteres speciaux\n",
    "        t['text'] = re.sub(r'\\xad', '-',\n",
    "                           re.sub(r'\\n', ' ', # retrait des sauts de ligne\n",
    "                                  re.sub(r'\\[a-zA-Z]*(?!\\S)', '', # retrait de ce qui n'est pas un mot\n",
    "                                         re.sub(r'(?:htt)\\S*', '', # retrait des liens http\n",
    "                                                re.sub(r'^rt.*: ', '', # retrait de la mention retweet\n",
    "                                                      re.sub(r'\\d', '', # retrait des chiffres\n",
    "                                                              re.sub(r',;!?\\/\\*(){}«»', ' ', t['text'])))))))\n",
    "        \n",
    "        t['text'] = re.sub('|'.join(['’', '_', '/', '-', '\\'', '“', '\\.']), ' ', t['text'])\n",
    "\n",
    "        # apostrophes\n",
    "        t['text'] = re.sub('|'.join([elem + '\\'' for elem in 'cdjlmnst']), '', t['text'])\n",
    "        negation = 0\n",
    "        adverbes = 0\n",
    "        for token in t['text'] : \n",
    "            if token in ['ne','pas','n\\'','n','sans','aucun','nul','nulle','rien','jamais','personne'] : \n",
    "                negation += 1\n",
    "            if re.match('\\w\\S*ment', token) is not None :\n",
    "                adverbes += 1        \n",
    "        tokenizer = TreebankWordTokenizer()\n",
    "        t['text'] = tokenizer.tokenize(t['text'])\n",
    "        t['text'] = [token for token in t['text'] if (token not in stops) and (len(token)>2)]\n",
    "\n",
    "                \n",
    "        while '' in t['text']:\n",
    "            t['text'].pop('')\n",
    "            \n",
    "        if t['text']: # test si liste non vide\n",
    "            listTweets.append(t['text'])\n",
    "            try:\n",
    "                listCandidats.append(t['candidat'])\n",
    "            except:\n",
    "                listCandidats.append(None)\n",
    "            \n",
    "            try:\n",
    "                listSentiments.append(t['sentiment'])\n",
    "            except:\n",
    "                listSentiments.append(None)\n",
    "                \n",
    "            rec = pd.DataFrame([[a, b, c, d, e, f,g, h, negation, adverbes]], \n",
    "                               columns=['!', '?', '#', '\"', '_http_',':','»','@','négation','adverbes'])\n",
    "            df = df.append(rec, ignore_index=True)\n",
    "        \n",
    "    df['text'], df['candidat'], df['sentiment'] = listTweets, listCandidats, listSentiments\n",
    "    return df\n",
    "\n",
    "def build_feat_mat(df_tweets):\n",
    "    vectorizer = TfidfVectorizer(strip_accents='unicode', analyzer='word', decode_error='strict',\n",
    "                                use_idf=True, norm='l2', binary=False, min_df=0.001, max_df=0.05, ngram_range=(1,10))\n",
    "    tfidf = vectorizer.fit_transform(df_tweets['text'].apply(' '.join))\n",
    "    X = pd.DataFrame(tfidf.toarray())\n",
    "    X[['!', '?', '#', '\"', '_http_',':','»','@','négation','adverbes']] = \\\n",
    "    df_tweets[['!', '?', '#', '\"', '_http_',':','»','@','négation','adverbes']]\n",
    "    return X\n",
    "\n",
    "\n",
    "def getSentiments(client, retweet, classifier, full_retrain=True): \n",
    "    if full_retrain :\n",
    "        # bases utilisées\n",
    "        df = tweetPreprocessing(client.tweet.train, 0, retweet)\n",
    "        df2 = tweetPreprocessing(client.tweet.labelised, 0, retweet)\n",
    "        try:\n",
    "            df = pd.concat([df, df2], axis=0, ignore_index=True)\n",
    "        except:\n",
    "            pass\n",
    "        df = df.sample(frac=1.0, replace=False) # mélange des lignes\n",
    "        X = build_feat_mat(df)\n",
    "        y = df['sentiment']\n",
    "        X_train,X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
    "        n_samples, vocabulaire = X.shape\n",
    "        print(\"Répartition dans le dataset de train : \\n\",\"\\tNégatif : \",\n",
    "              np.abs(np.sum(y_train[y_train == -1])/len(y_train)),\"%\\n\\tPositif : \",\n",
    "              np.abs(np.sum(y_train[y_train == 1])/len(y_train)),\"%\")\n",
    "        print(\"Répartition dans le dataset de test : \\n\",\"\\tNégatif : \",\n",
    "              np.abs(np.sum(y_test[y_test == -1])/len(y_test)),\"%\\n\\tPositif : \",\n",
    "              np.abs(np.sum(y_test[y_test == 1])/len(y_test)),\"%\")\n",
    "        print('Tweets : ' + str(n_samples) + ' / ' + 'Mots : ' + str(vocabulaire))\n",
    "        if classifier == 'lr':\n",
    "            params = {'penalty':['l2'], 'C' : np.linspace(0.6,0.75,30)}\n",
    "            clf = LogisticRegression(max_iter=2000, class_weight='balanced', multi_class='ovr')\n",
    "            \n",
    "        if classifier == 'svc':\n",
    "            params = {'C' : np.linspace(0.8, 1.1, 5)}\n",
    "            clf = SVC(class_weight='balanced', max_iter=2000, kernel='linear', decision_function_shape='ovr')\n",
    "         \n",
    "        if classifier == 'gb':\n",
    "            params = {'loss':['deviance','exponential'], 'learning_rate': np.logspace(-2,0,3),\n",
    "                     'n_estimators':range(150,200,10), 'max_depth':range(50,150,10)}\n",
    "            clf = GradientBoostingClassifier(max_features='auto')\n",
    "            \n",
    "        if classifier == 'mnb' :\n",
    "            params = {'alpha': np.linspace(0,1,10), 'fit_prior':['True','False'], 'class_prior':[[0.45, 0.3, 0.25], None]}\n",
    "            clf = MultinomialNB()\n",
    "            \n",
    "        GCV = GridSearchCV(clf, params, verbose=9, n_jobs=-1)\n",
    "        GCV.fit(X_train, y_train)\n",
    "        print(GCV.best_params_)\n",
    "        \n",
    "        \n",
    "        if classifier == 'lr' :\n",
    "            model = LogisticRegression(penalty=GCV.best_params_['penalty'],\n",
    "                                   C=GCV.best_params_['C'], max_iter=2000, multi_class='ovr', class_weight='balanced')\n",
    "        if classifier == 'svc' :\n",
    "            model = SVC(C=GCV.best_params_['C'], gamma=1e-15,\n",
    "                        decision_function_shape='ovr', class_weight='balanced')\n",
    "    \n",
    "        if classifier == 'mnb':\n",
    "            model = MultinomialNB(alpha=GCV.best_params_['alpha'], fit_prior=GCV.best_params_['fit_prior'], \n",
    "                                 class_prior=GCV.best_params_['class_prior'])\n",
    "    \n",
    "        if classifier == 'gb' :\n",
    "            model = GradientBoostingClassifier(loss=GCV.best_params_['loss'],\n",
    "                                               learning_rate=GCV.best_params_['learning_rate'],\n",
    "                                               n_estimators=GCV.best_params_['n_estimators'],\n",
    "                                               max_depth=GCV.best_params_['max_depth'],\n",
    "                                               max_features='auto')\n",
    "    \n",
    "    else :\n",
    "        f = open('sentiment_model.pkl','rb')\n",
    "        model = pickle.load(f)\n",
    "        f.close()\n",
    "        # bases utilisées\n",
    "        df = tweetPreprocessing(client.tweet.train, 500, retweet)\n",
    "        df2 = tweetPreprocessing(client.tweet.labelised, 500, retweet)\n",
    "        try:\n",
    "            df = pd.concat([df, df2], axis=0, ignore_index=True)\n",
    "        except:\n",
    "            pass\n",
    "        df = df.sample(frac=1.0, replace=False) # mélange des lignes\n",
    "        X = build_feat_mat(df)\n",
    "        y = df['sentiment']\n",
    "        n_samples, vocabulaire = X.shape\n",
    "        print('Tweets : ' + str(n_samples) + ' / ' + 'Mots : ' + str(vocabulaire))\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    print('Score', np.sum(predictions == y_test) / len(predictions)) \n",
    "    f = open('sentiment_model.pkl','wb')\n",
    "    pickle.dump(model, f)\n",
    "    f.close()\n",
    "    print('Model saved')\n",
    "    return predictions\n",
    "\n",
    "def cleanTrainDb(collection):\n",
    "    textCleanPipeline = [{\"$group\":{\"_id\":\"$text\", \"dups\":{\"$push\":\"$_id\"},\"count\":{\"$sum\":1}}},{\"$match\":{\"count\":{\"$gt\":1}}}]\n",
    "    duplicates = []\n",
    "    count = 0\n",
    "    try:\n",
    "        for doc in collection.aggregate(textCleanPipeline) :\n",
    "            it = iter(doc['dups'])\n",
    "            next(it)\n",
    "            for id in it:\n",
    "                count += 1\n",
    "                duplicates.append(pym.DeleteOne({'_id':id}))\n",
    "        if duplicates:\n",
    "            collection.bulk_write(duplicates)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print(count, 'doublons retirés.')\n",
    "    client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9887\n",
      "1421\n"
     ]
    }
   ],
   "source": [
    "client = pym.MongoClient('localhost', 27018)\n",
    "#client = pym.MongoClient('localhost', 27017)\n",
    "collection = client.tweet.train\n",
    "collection2 = client.tweet.labelised\n",
    "print(collection.count())\n",
    "print(collection2.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Test du nettoyage des tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "a = tweetPreprocessing(collection, retweet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Test du nettoyage doublons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 doublons retirés.\n",
      "0 doublons retirés.\n"
     ]
    }
   ],
   "source": [
    "cleanTrainDb(collection)\n",
    "cleanTrainDb(collection2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train et test du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Répartition dans le dataset de train : \n",
      " \tNégatif :  0.468355678233 %\n",
      "\tPositif :  0.228312302839 %\n",
      "Répartition dans le dataset de test : \n",
      " \tNégatif :  0.443262411348 %\n",
      "\tPositif :  0.256205673759 %\n",
      "Tweets : 11272 / Mots : 1452\n",
      "Fitting 3 folds for each of 300 candidates, totalling 900 fits\n",
      "[CV] n_estimators=150, learning_rate=0.01, max_depth=50, loss=deviance \n",
      "[CV] n_estimators=150, learning_rate=0.01, max_depth=50, loss=deviance \n",
      "[CV] n_estimators=150, learning_rate=0.01, max_depth=50, loss=deviance \n",
      "[CV]  n_estimators=150, learning_rate=0.01, max_depth=50, loss=deviance, score=0.558580, total=48.0min\n",
      "[CV] n_estimators=160, learning_rate=0.01, max_depth=50, loss=deviance \n",
      "[CV]  n_estimators=150, learning_rate=0.01, max_depth=50, loss=deviance, score=0.557067, total=48.4min\n",
      "[CV] n_estimators=160, learning_rate=0.01, max_depth=50, loss=deviance \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed: 48.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=150, learning_rate=0.01, max_depth=50, loss=deviance, score=0.550266, total=48.6min\n",
      "[CV] n_estimators=160, learning_rate=0.01, max_depth=50, loss=deviance \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-e4b76fe0025f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#c = getSentiments(client, retweet=True, classifier='mnb', full_retrain=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#print(len(c[c==1]), len(c[c==0]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetSentiments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretweet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_retrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-77-d62a32701049>\u001b[0m in \u001b[0;36mgetSentiments\u001b[0;34m(client, retweet, classifier, full_retrain)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mGCV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mGCV\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGCV\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/raphael/anaconda3/lib/python3.5/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    943\u001b[0m             \u001b[0mtrain\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtest\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \"\"\"\n\u001b[0;32m--> 945\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/raphael/anaconda3/lib/python3.5/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, groups, parameter_iterable)\u001b[0m\n\u001b[1;32m    562\u001b[0m                                   \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m                                   error_score=self.error_score)\n\u001b[0;32m--> 564\u001b[0;31m           \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m           for train, test in cv_iter)\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/raphael/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    766\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/raphael/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/raphael/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0;31m# check if timeout supported in backend future implementation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m'timeout'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetfullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/raphael/anaconda3/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/raphael/anaconda3/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/raphael/anaconda3/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/raphael/anaconda3/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#a = getSentiments(client, retweet=True, classifier='lr', full_retrain=True)\n",
    "# attention en cas de retrain périodique on ne prend que les 500 derniers tweets, il faut donc prédire sur moins de 500\n",
    "#print(len(a[a==1]), len(a[a==0]))\n",
    "#b = getSentiments(client, retweet=True, classifier='svc', full_retrain=True)\n",
    "#print(len(b[b==1]), len(b[b==0]))\n",
    "#c = getSentiments(client, retweet=True, classifier='mnb', full_retrain=True)\n",
    "#print(len(c[c==1]), len(c[c==0]))\n",
    "d = getSentiments(client, retweet=True, classifier='gb', full_retrain=True)\n",
    "print(len(d[d==1]), len(d[d==0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Essai sur un Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def TrainXGBoostModel(client, retweet=True, gpuEnabled=False):\n",
    "    df = tweetPreprocessing(client.tweet.train, 0, retweet)\n",
    "    df2 = tweetPreprocessing(client.tweet.labelised, 0, retweet)\n",
    "    try:\n",
    "        df = pd.concat([df, df2], axis=0, ignore_index=True)\n",
    "    except:\n",
    "        pass\n",
    "    df = df.sample(frac=1.0, replace=False) # mélange des lignes\n",
    "    X = build_feat_mat(df)\n",
    "    df[df['sentiment'] == -1] = 2\n",
    "    y = df['sentiment']\n",
    "    x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.1) # 10 % for validation\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.1) # 10 % for test\n",
    "    n_samples, vocabulaire = X.shape\n",
    "    print('Tweets : ' + str(n_samples) + ' / ' + 'Mots : ' + str(vocabulaire))\n",
    "\n",
    "    boostRounds = 10\n",
    "    # XGBoost data structure\n",
    "    xgtrain = xgb.DMatrix(x_train.values, y_train.values)\n",
    "    xgeval = xgb.DMatrix(x_val.values, y_val.values)\n",
    "    xgtest = xgb.DMatrix(x_test.values)\n",
    "    params = {'eta': 0.01, 'gamma' : 0.2, 'max_depth': 200, 'num_class':3,\n",
    "              'lambda': 3, 'alpha': 0.05, 'objective':'multi:softmax', 'eval_metric':'merror', 'eval_metric':'merror'}\n",
    "     \n",
    "    \n",
    "    # Generate the watchlist\n",
    "    watchlist = [(xgtrain, \"train\"), (xgeval, \"eval\")]\n",
    "    print(\"Train the XGBoost model...\")\n",
    "    startTime = time.time()\n",
    "    clf = xgb.train(params, xgtrain, boostRounds, evals = watchlist, \n",
    "                    maximize = True, verbose_eval = 100)\n",
    "\n",
    "    #Make predictions\n",
    "    print(\"Make predictions for testing set...\")\n",
    "    predictions_sr = pd.Series(clf.predict(xgtest, ntree_limit = clf.best_iteration))\n",
    "    return predictions_sr, clf, y_test, x_train.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets : 7791 / Mots : 13133\n",
      "Train the XGBoost model...\n",
      "[0]\ttrain-merror:0.183864\teval-merror:0.423077\n",
      "Make predictions for testing set...\n"
     ]
    }
   ],
   "source": [
    "preds, model,  truth = TrainXGBoostModel(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
