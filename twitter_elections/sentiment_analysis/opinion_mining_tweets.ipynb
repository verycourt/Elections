{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Approche : mélanger analyse lexicale et par apprentissage.\n",
    "\n",
    "1. Analyse lexicale des tweets :\n",
    "    - POS tagging, lemmisation\n",
    "    - Traduction des mots en Anglais pour pouvoir utiliser Sentiwordnet et obtenir la polarité des mots\n",
    "\n",
    "2. Application d'un modèle d'apprentissage supervisé :\n",
    "    - HMM, SVM, etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lien vers les POS tags en français : http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/data/french-tagset.html\n",
    "\n",
    "Etudier la possibilité d'ajout de lexiques d'opinion en Français : http://alpage.inria.fr/~sagot/wolf.html, http://sites.univ-provence.fr/wpsycle/outils_recherche/liwc/FrenchLIWCDictionary_V1_1.dic, http://sites.univ-provence.fr/~wpsycle/outils_recherche/outils_recherche.html#emotaix\n",
    "\n",
    "Voir l'approche compositionnelle : Moilanen 2007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "import treetaggerwrapper\n",
    "import time\n",
    "\n",
    "import pymongo as pym\n",
    "#import nltk.data\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import stop_words\n",
    "#from nltk.stem import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stops = set(['rt','ds','qd','ss','ns','vs','nn','amp','gt','gd','gds','tt','pr','ac','mm', 'qu',\n",
    "            '``', 'ni', 'ca', 'le', 'les', ' ', 'si', '$', '^', 'via', 'ils','pour','une','que','quel']\n",
    "#            + list('@ن%£€‘:&;')\n",
    "            + list('abcdefghijklmnopqrstuvwxyzà'))\n",
    "\n",
    "#            stop_words.get_stop_words(language='fr')+stopwords.words('french'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stops = set(list('abcdefghijklmnopqrstuvwxyzà'))\n",
    "# TODO: améliorer la liste de stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mongo_to_df(collection, n_last_tweets=0, retweet=False):\n",
    "    tweets = collection.find(filter={'text':{'$exists':True}}, \n",
    "                             projection={'_id':False}).sort('$natural',-1).limit(n_last_tweets)\n",
    "    df = pd.DataFrame()\n",
    "    listTweets, listCandidats, listSentiments = [], [], []\n",
    "    \n",
    "    for t in tweets: \n",
    "        if not retweet: # filtrage des retweets\n",
    "            if 'rt @' in t['text']:\n",
    "                continue\n",
    "\n",
    "        if t['text']: # test si liste non vide\n",
    "            listTweets.append(t['text'])\n",
    "            try:\n",
    "                listCandidats.append(t['candidat'])\n",
    "            except:\n",
    "                listCandidats.append(None)\n",
    "            \n",
    "            try:\n",
    "                listSentiments.append(t['sentiment'])\n",
    "            except:\n",
    "                listSentiments.append(None)\n",
    "    \n",
    "    df['text'], df['candidat'], df['sentiment'] = listTweets, listCandidats, listSentiments\n",
    "    return df\n",
    "\n",
    "def regex_filter(text):\n",
    "    text = re.sub(r'\\w*…', '', text) # mot tronqué par Twitter\n",
    "    text = re.sub(r'(?:htt)\\S*', '', text) # retrait des liens http\n",
    "    text = re.sub(r'\\n', ' ', text) # retrait des sauts de ligne\n",
    "    text = re.sub(r'\\xad', '-', text)\n",
    "    text = re.sub(r'@\\w*', '', text) # retrait des mentions @ (ne détecte pas @XXX@...)\n",
    "    text = re.sub(r'\\.{3,}', '...', text) # ....... => points de suspension\n",
    "    text = re.sub(r'(?=\\.\\w)(\\.)', '. ', text) # remplacer un point entre deux mots 'A.B' par 'A. B'\n",
    "    #text = re.sub(r'\\[a-zA-Z]*(?!\\S)', '', text) # retrait de ce qui n'est pas un mot\n",
    "    #text = re.sub(r'^rt.*: ', '', text) # retrait de la mention retweet\n",
    "    #text = re.sub(r'\\d', '', text) # retrait des chiffres\n",
    "    #text = re.sub(r',;!?\\/\\*(){}«»', ' ', text)\n",
    "    #text = re.sub('|'.join(['’', '_', '/', '-', '\\'', '“', '\\.']), ' ', text)\n",
    "    #text = re.sub('|'.join([elem + '\\'' for elem in 'cdjlmnst']), '', text) # apostrophes\n",
    "    return text\n",
    "\n",
    "def process_texts(list_of_texts):\n",
    "    # Processing the tweets (POS tagging, lemmatization, spellchecking)\n",
    "    tagger = treetaggerwrapper.TreeTagger(TAGLANG='fr')\n",
    "    list_of_processed_texts = []\n",
    "    \n",
    "    for text in list_of_texts:\n",
    "        text = regex_filter(text)\n",
    "        \n",
    "        # TODO: (optionnel) retirer les # avant d'utiliser TreeTagger\n",
    "        # puis les remettre avec le tag HASH|\n",
    "        \n",
    "        # TODO: correction orthographique des tweets\n",
    "        \n",
    "        tags = tagger.tag_text(text)\n",
    "        try:\n",
    "            tagged_text = ['{}|{}'.format(t.split('\\t')[1], t.split('\\t')[2]) for t in tags\n",
    "                           if t.split('\\t')[2] not in stops] # ou bien filtrer sur le POS tag\n",
    "        except:\n",
    "            tagged_text = ['ERREUR']\n",
    "        \n",
    "        # append les HASH|#...\n",
    "        # TODO: (optionnel) gérer les accents sur les mots\n",
    "        list_of_processed_texts.append(tagged_text)\n",
    "    return list_of_processed_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test du TreeTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['exemple\\tNOM\\texemple', 'de\\tPRP\\tde', 'texte\\tNOM\\ttexte', 'à\\tPRP\\tà', 'taguer\\tNOM\\ttaguer']\n",
      "['NOM|exemple', 'PRP|de', 'NOM|texte', 'PRP|à', 'NOM|taguer']\n"
     ]
    }
   ],
   "source": [
    "tagger = treetaggerwrapper.TreeTagger(TAGLANG='fr')\n",
    "tags = tagger.tag_text('exemple de texte à taguer')\n",
    "print(tags)\n",
    "tagged_text = ['{}|{}'.format(t.split('\\t')[1], t.split('\\t')[2]) for t in tags]\n",
    "pprint.pprint(tagged_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "client = pym.MongoClient('localhost',27017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base annotée manuellement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9912 tweets in the manual train set.\n",
      "-1.0    5275\n",
      " 0.0    3437\n",
      " 1.0    1199\n",
      "Name: sentiment, dtype: int64\n",
      "Series([], Name: candidat, dtype: int64)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>candidat</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ben voyons donc ! \"par erreur\" aussi ils nous ...</td>\n",
       "      <td>None</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>l'écologie version macron : les contradictions...</td>\n",
       "      <td>None</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@ericwoerth : \"françois #fillon est audible : ...</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fillon fait honte au monde entier mais il s ac...</td>\n",
       "      <td>None</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@gg_rmc @bayrou la girouette de la politique-l...</td>\n",
       "      <td>None</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text candidat  sentiment\n",
       "0  ben voyons donc ! \"par erreur\" aussi ils nous ...     None       -1.0\n",
       "1  l'écologie version macron : les contradictions...     None       -1.0\n",
       "2  @ericwoerth : \"françois #fillon est audible : ...     None        1.0\n",
       "3  fillon fait honte au monde entier mais il s ac...     None       -1.0\n",
       "4  @gg_rmc @bayrou la girouette de la politique-l...     None       -1.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection = client.tweet.train\n",
    "print('{} tweets in the manual train set.'.format(collection.count()))\n",
    "df_tweets = mongo_to_df(collection, n_last_tweets=0, retweet=True)\n",
    "print(df_tweets['sentiment'].value_counts())\n",
    "print(df_tweets['candidat'].value_counts())\n",
    "df_tweets.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base annotée automatiquement, sur la base des hashtags (uniquement des tweets positifs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1421 tweets in the auto train set.\n",
      "1    1418\n",
      "Name: sentiment, dtype: int64\n",
      "fillon       299\n",
      "macron       289\n",
      "le pen       288\n",
      "hamon        274\n",
      "melenchon    268\n",
      "Name: candidat, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>candidat</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>même s'il veut faire croire qu'il l'adoucit le...</td>\n",
       "      <td>melenchon</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>si #fillon pas présent second tour #mlp sera m...</td>\n",
       "      <td>melenchon</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#fillon et #macron sont au coude à coude dans ...</td>\n",
       "      <td>melenchon</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@edfofficiel votre pub est à chier, où avait v...</td>\n",
       "      <td>melenchon</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sofiakkar faut pas pousser  #fillon  parle de...</td>\n",
       "      <td>melenchon</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   candidat  sentiment\n",
       "0  même s'il veut faire croire qu'il l'adoucit le...  melenchon          1\n",
       "1  si #fillon pas présent second tour #mlp sera m...  melenchon          1\n",
       "2  #fillon et #macron sont au coude à coude dans ...  melenchon          1\n",
       "3  @edfofficiel votre pub est à chier, où avait v...  melenchon          1\n",
       "4  @sofiakkar faut pas pousser  #fillon  parle de...  melenchon          1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection = client.tweet.labelised\n",
    "print('{} tweets in the auto train set.'.format(collection.count()))\n",
    "df_tweets_auto = mongo_to_df(collection, n_last_tweets=0, retweet=False)\n",
    "print(df_tweets_auto['sentiment'].value_counts())\n",
    "print(df_tweets_auto['candidat'].value_counts())\n",
    "df_tweets_auto.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TreeTagger a renvoyé 0 erreur(s).\n",
      "     0    1    2    3    4    5    6    7    8    9    ...     11797  11798  \\\n",
      "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0    ...       0.0    0.0   \n",
      "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0    ...       0.0    0.0   \n",
      "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0    ...       0.0    0.0   \n",
      "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0    ...       0.0    0.0   \n",
      "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0    ...       0.0    0.0   \n",
      "\n",
      "   11799  11800  11801  #  http  @     n_car   n_words  \n",
      "0    0.0    0.0    0.0  0     0  3  4.828314  2.833213  \n",
      "1    0.0    0.0    0.0  0     1  1  4.753590  2.772589  \n",
      "2    0.0    0.0    0.0  1     2  0  4.919981  2.890372  \n",
      "3    0.0    0.0    0.0  1     1  0  4.919981  3.091042  \n",
      "4    0.0    0.0    0.0  0     1  0  4.465908  2.484907  \n",
      "\n",
      "[5 rows x 11807 columns]\n"
     ]
    }
   ],
   "source": [
    "# Tweet feature extraction\n",
    "hashtag = [t.count('#') for t in df_tweets['text']]\n",
    "links = [t.count('http') for t in df_tweets['text']]\n",
    "at = [t.count('@') for t in df_tweets['text']]\n",
    "n_car = [np.log(len(t)) for t in df_tweets['text']]\n",
    "n_words = [np.log(len(t.split(' '))) for t in df_tweets['text']]\n",
    "\n",
    "try:\n",
    "    hashtag.extend([t.count('#') for t in df_tweets_auto['text']])\n",
    "    links.extend([t.count('http') for t in df_tweets_auto['text']])\n",
    "    at.extend([t.count('@') for t in df_tweets_auto['text']])\n",
    "    n_car.extend([np.log(len(t)) for t in df_tweets_auto['text']])\n",
    "    n_words.extend([np.log(len(t.split(' '))) for t in df_tweets_auto['text']])\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# a = t['text'].count('!')\n",
    "# b = t['text'].count('?')\n",
    "# c = t['text'].count('#')\n",
    "# d = t['text'].count('\"')\n",
    "# e = t['text'].count('http')\n",
    "# f = t['text'].count(':')\n",
    "# g = t['text'].count('»')\n",
    "# h = t['text'].count('@')\n",
    "\n",
    "# Tweet processing\n",
    "tweet_list = process_texts(df_tweets['text'])\n",
    "try:\n",
    "    tweet_list.extend(process_texts(df_tweets_auto['text']))\n",
    "except:\n",
    "    pass\n",
    "print('TreeTagger a renvoyé {} erreur(s).'.format(tweet_list.count('ERREUR')))\n",
    "\n",
    "# Building feature matrix\n",
    "vectorizer = TfidfVectorizer(strip_accents=None, analyzer='word', decode_error='strict',\n",
    "                            use_idf=False, norm=None, binary=False, min_df=1, max_df=1.0, ngram_range=(1,1))\n",
    "tfidf = vectorizer.fit_transform([' '.join(tweet) for tweet in tweet_list])\n",
    "X = pd.DataFrame(tfidf.toarray())\n",
    "X['#'], X['http'], X['@'], X['n_car'], X['n_words'] = hashtag, links, at, n_car, n_words\n",
    "print(X[:5])\n",
    "\n",
    "try:\n",
    "    y = pd.concat([df_tweets['sentiment'], df_tweets_auto['sentiment']], axis=0)\n",
    "except:\n",
    "    y = df_tweets['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Répartition dans le dataset de train (9054 tweets) : \n",
      " \tNégatif : 0.464877402253 %\n",
      "\tPositif : 0.232273028496 %\n",
      "Répartition dans le dataset de test (2264 tweets) : \n",
      " \tNégatif : 0.468197879859 %\n",
      "\tPositif : 0.225265017668 %\n",
      "Tweets : 11318 / N-grams : 11807\n",
      "\n",
      "Score 0.67093639576\n",
      "Répartition des prédictions : \n",
      " \tNégatif : 0.469081272085 %\n",
      "\tPositif : 0.191696113074 %\n"
     ]
    }
   ],
   "source": [
    "# Building train & test sets\n",
    "X_train,X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "n_samples, vocabulaire = X.shape\n",
    "\n",
    "print(\"Répartition dans le dataset de train ({} tweets) : \\n\".format(len(y_train)),\n",
    "      \"\\tNégatif :\", np.abs(np.sum(y_train[y_train == -1])/len(y_train)),\n",
    "      \"%\\n\\tPositif :\", np.abs(np.sum(y_train[y_train == 1])/len(y_train)),\"%\")\n",
    "print(\"Répartition dans le dataset de test ({} tweets) : \\n\".format(len(y_test)),\n",
    "      \"\\tNégatif :\", np.abs(np.sum(y_test[y_test == -1])/len(y_test)),\n",
    "      \"%\\n\\tPositif :\", np.abs(np.sum(y_test[y_test == 1])/len(y_test)),\"%\")\n",
    "print('Tweets : ' + str(n_samples) + ' / ' + 'N-grams : ' + str(vocabulaire))\n",
    "\n",
    "# Choise of models\n",
    "clf = LogisticRegression(penalty='l2', C=1., max_iter=2000, class_weight='balanced', multi_class='ovr')\n",
    "#clf = SVC(C=1.0, class_weight='balanced', max_iter=2000, kernel='linear', decision_function_shape='ovr')\n",
    "#clf = LinearSVC(C=.5, dual=True, class_weight='balanced')\n",
    "#clf = MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)\n",
    "\n",
    "# Fit & predict\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "print('\\nScore', np.sum(predictions == y_test) / len(predictions))\n",
    "\n",
    "print('Répartition des prédictions : \\n',\n",
    "      '\\tNégatif :', np.abs(np.sum(predictions[predictions == -1])/len(predictions)),\n",
    "      '%\\n\\tPositif :', np.abs(np.sum(predictions[predictions == 1])/len(predictions)), '%')\n",
    "\n",
    "# TODO: ajouter matrice de confusion, score f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
