{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# coding: utf-8\n",
    "\n",
    "from datetime import date, datetime, timedelta\n",
    "from predict_functions import *\n",
    "from sklearn.externals import joblib\n",
    "import pandas as pd\n",
    "import pymongo as pym\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Chargement des tweets des candidats depuis MongoDB pour le 2017-04-22...\n",
      "------------------------------\n",
      "95845 tweets trouves.\n",
      "Tagging des tweets en cours...\n",
      "TreeTagger a renvoye 0 erreur(s).\n",
      "Creation de la matrice de features...\n",
      "Taille du vocabulaire : 4884\n",
      "52249 documents vectorises.\n",
      "Prediction des tweets...\n",
      "------------------------------\n",
      "Chargement des tweets des candidats depuis MongoDB pour le 2017-04-21...\n",
      "------------------------------\n",
      "167605 tweets trouves.\n",
      "Tagging des tweets en cours...\n",
      "TreeTagger a renvoye 0 erreur(s).\n",
      "Creation de la matrice de features...\n",
      "Taille du vocabulaire : 4884\n",
      "98233 documents vectorises.\n",
      "Prediction des tweets...\n",
      "                         count       neg       neu       pos\n",
      "date       candidat                                         \n",
      "2017-04-21 arthaud       103.0  0.000000  0.980583  0.019417\n",
      "           asselineau    424.0  0.162736  0.806604  0.030660\n",
      "           cheminade      63.0  0.380952  0.619048  0.000000\n",
      "           fillon      35709.0  0.535887  0.416954  0.047159\n",
      "           hamon        4427.0  0.314434  0.484979  0.200587\n",
      "           lassalle       26.0  0.923077  0.076923  0.000000\n",
      "           le pen      27813.0  0.808794  0.177651  0.013555\n",
      "           macron      27027.0  0.710142  0.254338  0.035520\n",
      "           melenchon    3257.0  0.252072  0.425238  0.322690\n",
      "           nda          2147.0  0.648347  0.309269  0.042385\n",
      "           poutou        950.0  0.053684  0.873684  0.072632\n",
      "2017-04-22 arthaud        46.0  0.021739  0.565217  0.413043\n",
      "           asselineau    137.0  0.240876  0.744526  0.014599\n",
      "           cheminade       8.0  0.375000  0.500000  0.125000\n",
      "           fillon      21574.0  0.479281  0.483406  0.037313\n",
      "           hamon        2396.0  0.408180  0.303840  0.287980\n",
      "           lassalle        3.0  1.000000  0.000000  0.000000\n",
      "           le pen       8883.0  0.648880  0.291343  0.059777\n",
      "           macron      17081.0  0.765646  0.192260  0.042094\n",
      "           melenchon    2315.0  0.298488  0.281210  0.420302\n",
      "           nda          1619.0  0.433601  0.542928  0.023471\n",
      "           poutou        439.0  0.063781  0.763098  0.173121\n",
      "Sauvegarde des pourcentages par candidat dans un .csv : data/twitter_sentiments_21_au_22_.csv\n"
     ]
    }
   ],
   "source": [
    "# les fonctions appelées par ce notebook se trouvent dans le fichier python predict_functions\n",
    "\n",
    "# les paramètres\n",
    "# date = datetime.strftime(datetime.utcnow() - timedelta(hours=24), '%Y-%m-%d')\n",
    "# dates = [('2017-04-' + str(d)) if d > 9 else ('2017-04-0' + str(d)) for d in range(5, 0, -1)]\n",
    "dates = ['2017-04-22', '2017-04-21']\n",
    "fname = 'data/twitter_sentiments_21_au_22_.csv'\n",
    "\n",
    "df_pred = pd.DataFrame()\n",
    "\n",
    "# 1. Chargement du vocabulaire et du modèle\n",
    "voca = pd.read_json('trained_dict.json').to_dict()[0]\n",
    "clf = joblib.load('trained_logistic_regression.pkl')\n",
    "\n",
    "other_politicians = ['valls', 'sarko', 'hollande']\n",
    "candidates = {'macron': 'macron|emmanuel',\n",
    "              'fillon': 'fillon',\n",
    "              'hamon': 'hamon|benoit|benoît',\n",
    "              'melenchon': 'melenchon|mélenchon|jlm',\n",
    "              'le pen': 'le pen|lepen|mlp|marine',\n",
    "              'nda': 'aignan|dupont|nda',\n",
    "              'asselineau': 'asselineau|asselinau|aselineau',\n",
    "              'lassalle': 'lassalle|lassale|lasalle|lasale',\n",
    "              'poutou': 'poutou',\n",
    "              'arthaud': 'arthaud',\n",
    "              'cheminade': 'cheminade'\n",
    "             }\n",
    "stop_words = '|'.join([pol for pol in other_politicians])\n",
    "    \n",
    "for date in dates:\n",
    "    try:\n",
    "        # 2. Chargement des tweets a predire\n",
    "        # dataframe contenant les tweets a predire dans une colonne 'text'\n",
    "        print(30*'-')\n",
    "        print('Chargement des tweets des candidats depuis MongoDB pour le {}...'.format(date))\n",
    "        print(30*'-')\n",
    "        df = extract_tweets(date, days=1, port=27017, limit=250000) # limite pour éviter les MemoryError\n",
    "\n",
    "        if df.shape[0]:\n",
    "            # on repère les tweets où plusieurs candidats sont cités\n",
    "            df['other'] = df['text'].str.contains(stop_words, case=False) \n",
    "\n",
    "            # on repère les candidats contenus dans les tweets\n",
    "            for candidate in candidates:\n",
    "                df[candidate] = df['text'].str.contains(candidate, case=False)\n",
    "\n",
    "            # filtrage des tweets contenant d'autres personnalités politiques\"'\n",
    "            df = df[df['other']==False]\n",
    "\n",
    "            # filtrage des tweets contenant plusieurs des 5 candidats (ou aucun candidat)\n",
    "            df['count'] = 1 * df.fillon + df.macron + df['le pen'] + df.melenchon + df.hamon\n",
    "            df = df[df['count']==1]\n",
    "            df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            # 3. Creation des features et de la matrice TF-IDF pour la base test\n",
    "            X_test = build_X(df, drop_dups=False, vocab=voca, min_df=3, n_grams=(1,1))\n",
    "\n",
    "            # 4. Prediction\n",
    "            print('Prediction des tweets...')\n",
    "            y_pred = clf.predict(X_test)\n",
    "            del X_test\n",
    "            df['sentiment'] = y_pred\n",
    "            \n",
    "#             try:\n",
    "#                 print('Insertion dans la base MongoDB \"predicted\"...')\n",
    "#                 insert_in_mongo(df.drop(['other', 'count'], axis=1), port=27017)\n",
    "#             except:\n",
    "#                 print('Echec de l\\'insertion dans la base MongoDB')\n",
    "\n",
    "            # 5. Sauvegarder les predictions\n",
    "            # ajout de la ligne du candidat dans le dataframe\n",
    "            for candidate in candidates:\n",
    "                curr_df = df[df[candidate]==True]\n",
    "                taille = curr_df.shape[0]\n",
    "                rec = {'count': taille, 'candidat': candidate, 'date': date}\n",
    "                try:\n",
    "                    rec['neg'] = curr_df[curr_df['sentiment']==-1].shape[0] / taille\n",
    "                    rec['neu'] = curr_df[curr_df['sentiment']==0].shape[0] / taille\n",
    "                    rec['pos'] = curr_df[curr_df['sentiment']==1].shape[0] / taille\n",
    "                except:\n",
    "                    # si aucun tweet pour le candidat courant n'est dans la base (cas pathologique)\n",
    "                    rec['neg'], rec['neu'], rec['pos'] = ('-', '-', '-')\n",
    "\n",
    "                df_pred = df_pred.append(rec, verify_integrity=False, ignore_index=True)\n",
    "            \n",
    "            del df\n",
    "    except:\n",
    "        print('Erreur pour la date {}'.format(date))\n",
    "        del X_test\n",
    "        del df\n",
    "        continue\n",
    "        \n",
    "df_pred.set_index(['date', 'candidat'], drop=True, inplace=True)\n",
    "df_pred.sort_index(axis=0, level=[0,1], inplace=True)\n",
    "print(df_pred)\n",
    "\n",
    "print('Sauvegarde des pourcentages par candidat dans un .csv : {}'.format(fname))\n",
    "df_pred.to_csv(fname, index_label=('date','candidat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csv_file = pd.DataFrame(pd.read_csv(fname, sep = ',', header = 0, index_col = False))\n",
    "csv_file.to_json('twitter_sentiments_daily_21_until_22_april_raw.json', orient ='split')\n",
    "df = pd.read_json('twitter_sentiments_daily_21_until_22_april_raw.json', orient='split')\n",
    "df['neu'] = pd.to_numeric(df['neu'], errors='coerce')\n",
    "df['neg'] = pd.to_numeric(df['neg'], errors='coerce')\n",
    "df['pos'] = pd.to_numeric(df['pos'], errors='coerce')\n",
    "df = df.fillna(0)\n",
    "df['score'] = (df['pos']) / (df['neg'] + df['neu'] + df['pos'])\n",
    "df = df.fillna(0)\n",
    "df['score'] = df['score'] * df['count']\n",
    "df_pivot = df.pivot_table(index='date',columns='candidat',values='score').fillna(0)\n",
    "df_pivot.to_json('twitter_sentiments_daily_21_until_22_april.json', orient='split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
