{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# coding: utf-8\n",
    "\n",
    "from datetime import date, datetime, timedelta\n",
    "from predict_functions import *\n",
    "from sklearn.externals import joblib\n",
    "import pandas as pd\n",
    "import pymongo as pym\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des tweets des candidats depuis la base MongoDB ...\n",
      "106972 tweets trouves.\n",
      "Tagging des tweets en cours...\n",
      "TreeTagger a renvoye 0 erreur(s).\n",
      "Creation de la matrice de features...\n",
      "Longueur du vocabulaire : 4794\n",
      "52458 documents vectorises.\n",
      "Prediction des tweets...\n",
      "Insertion dans la base MongoDB \"predicted\"...\n",
      "Sauvegarde des pourcentages par candidat dans un .json : data/twitter_sentiments_2017-04-04.json\n",
      "             count       neg       neu       pos\n",
      "candidat                                        \n",
      "melenchon   1459.0  0.519534  0.272790  0.207676\n",
      "fillon     19255.0  0.532329  0.372319  0.095352\n",
      "macron     23661.0  0.581252  0.341997  0.076751\n",
      "le pen      3619.0  0.366952  0.538823  0.094225\n",
      "hamon       4464.0  0.186604  0.722446  0.090950\n"
     ]
    }
   ],
   "source": [
    "# les fonctions appelées par ce notebook se trouvent dans le fichier python predict_functions\n",
    "\n",
    "date = datetime.strftime(datetime.utcnow() - timedelta(hours=24), '%Y-%m-%d')\n",
    "fname = 'data/twitter_sentiments_{}.json'.format(date)\n",
    "\n",
    "df_pred = pd.DataFrame()\n",
    "\n",
    "# 1. Chargement du vocabulaire\n",
    "voca = pd.read_json('trained_dict.json').to_dict()[0]\n",
    "\n",
    "# 2. Chargement des tweets a predire\n",
    "\n",
    "# dataframe contenant les tweets a predire dans une colonne 'text'\n",
    "print('Chargement des tweets des candidats depuis la base MongoDB ...')\n",
    "df = extract_tweets(date, days=1, port=27017)\n",
    "\n",
    "other_politicians = ['bayrou', 'aignan', 'poutou', 'arthaud', 'cheminade', 'valls', 'sarko', 'hollande']\n",
    "candidates = {'macron': 'macron|emmanuel',\n",
    "              'fillon': 'fillon',\n",
    "              'hamon': 'hamon|benoit|benoît',\n",
    "              'melenchon': 'melenchon|mélenchon|jlm',\n",
    "              'le pen': 'le pen|lepen|mlp|marine'\n",
    "             }\n",
    "\n",
    "# on repère les tweets où plusieurs candidats sont cités\n",
    "stop_words = '|'.join([pol for pol in other_politicians])\n",
    "df['other'] = df['text'].str.contains(stop_words, case=False) \n",
    "\n",
    "# on repère les candidats contenus dans les tweets\n",
    "for candidate in candidates:\n",
    "    df[candidate] = df['text'].str.contains(candidate, case=False)\n",
    "    \n",
    "# filtrage des tweets contenant d'autres personnalités politiques\n",
    "df = df[df['other']==False]\n",
    "\n",
    "# filtrage des tweets contenant plusieurs des 5 candidats (ou aucun candidat)\n",
    "df['count'] = 1 * df.fillon + df.macron + df['le pen'] + df.melenchon + df.hamon\n",
    "df = df[df['count']==1]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# 3. Creation des features et de la matrice TF-IDF pour la base test\n",
    "X_test, _, _ = build_Xy(df, drop_dups=False, vocab=voca, min_df=3, n_grams=(1,1))\n",
    "\n",
    "# 4. Chargement du modele entraine\n",
    "clf = joblib.load('trained_logistic_regression.pkl')\n",
    "\n",
    "# 5. Prediction\n",
    "print('Prediction des tweets...')\n",
    "y_pred = clf.predict(X_test)\n",
    "df['sentiment'] = y_pred\n",
    "\n",
    "# 6. Sauvegarder les predictions\n",
    "\n",
    "# ajout de la ligne du candidat dans le dataframe\n",
    "for candidate in candidates:\n",
    "    curr_df = df[df[candidate]==True]\n",
    "    taille = curr_df.shape[0]\n",
    "    rec = {'count': taille, 'candidat': candidate}\n",
    "    try:\n",
    "        rec['neg'] = curr_df[curr_df['sentiment']==-1].shape[0] / taille\n",
    "        rec['neu'] = curr_df[curr_df['sentiment']==0].shape[0] / taille\n",
    "        rec['pos'] = curr_df[curr_df['sentiment']==1].shape[0] / taille\n",
    "    except:\n",
    "        # si aucun tweet pour le candidat courant n'est dans la base\n",
    "        rec['neg'], rec['neu'], rec['pos'] = ('-', '-', '-')\n",
    "    \n",
    "    df_pred = df_pred.append(rec, verify_integrity=False, ignore_index=True)\n",
    "df_pred.set_index('candidat', drop=True, inplace=True)\n",
    "\n",
    "print('Sauvegarde des pourcentages par candidat dans un .json : {}'.format(fname))\n",
    "print(df_pred)\n",
    "df_pred.to_json(fname)\n",
    "\n",
    "print('Insertion dans la base MongoDB \"predicted\"...')\n",
    "insert_in_mongo(df.drop(['other', 'count'], axis=1), port=27017)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
