{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# coding: utf-8\n",
    "\n",
    "from datetime import date, datetime, timedelta\n",
    "from predict_functions import *\n",
    "from sklearn.externals import joblib\n",
    "import pandas as pd\n",
    "import pymongo as pym\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des tweets des candidats depuis la base MongoDB ...\n",
      "500 tweets trouves.\n",
      "Tagging des tweets en cours...\n",
      "TreeTagger a renvoye 0 erreur(s).\n",
      "Creation de la matrice de features...\n",
      "Taille du vocabulaire : 4794\n",
      "280 documents vectorises.\n",
      "Prediction des tweets...\n",
      "           count       neg       neu       pos\n",
      "candidat                                      \n",
      "fillon     183.0  0.907104  0.000000  0.092896\n",
      "macron      63.0  0.873016  0.079365  0.047619\n",
      "melenchon   10.0  0.400000  0.000000  0.600000\n",
      "le pen      14.0  0.857143  0.071429  0.071429\n",
      "hamon       10.0  0.600000  0.000000  0.400000\n"
     ]
    }
   ],
   "source": [
    "# les fonctions appelées par ce notebook se trouvent dans le fichier python predict_functions\n",
    "\n",
    "date = datetime.strftime(datetime.utcnow() - timedelta(hours=24), '%Y-%m-%d')\n",
    "fname = 'data/twitter_sentiments_{}.json'.format(date)\n",
    "\n",
    "df_pred = pd.DataFrame()\n",
    "\n",
    "# 1. Chargement du vocabulaire\n",
    "voca = pd.read_json('trained_dict.json').to_dict()[0]\n",
    "\n",
    "# 2. Chargement des tweets a predire\n",
    "# dataframe contenant les tweets a predire dans une colonne 'text'\n",
    "print('Chargement des tweets des candidats depuis la base MongoDB ...')\n",
    "df = extract_tweets(date, days=1, port=27017, limit=500)\n",
    "\n",
    "other_politicians = ['bayrou', 'aignan', 'poutou', 'arthaud', 'cheminade', 'valls', 'sarko', 'hollande']\n",
    "candidates = {'macron': 'macron|emmanuel',\n",
    "              'fillon': 'fillon',\n",
    "              'hamon': 'hamon|benoit|benoît',\n",
    "              'melenchon': 'melenchon|mélenchon|jlm',\n",
    "              'le pen': 'le pen|lepen|mlp|marine'\n",
    "             }\n",
    "\n",
    "# on repère les tweets où plusieurs candidats sont cités\n",
    "stop_words = '|'.join([pol for pol in other_politicians])\n",
    "df['other'] = df['text'].str.contains(stop_words, case=False) \n",
    "\n",
    "# on repère les candidats contenus dans les tweets\n",
    "for candidate in candidates:\n",
    "    df[candidate] = df['text'].str.contains(candidate, case=False)\n",
    "    \n",
    "# filtrage des tweets contenant d'autres personnalités politiques\n",
    "df = df[df['other']==False]\n",
    "\n",
    "# filtrage des tweets contenant plusieurs des 5 candidats (ou aucun candidat)\n",
    "df['count'] = 1 * df.fillon + df.macron + df['le pen'] + df.melenchon + df.hamon\n",
    "df = df[df['count']==1]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# 3. Creation des features et de la matrice TF-IDF pour la base test\n",
    "X_test = build_X(df, drop_dups=False, vocab=voca, min_df=3, n_grams=(1,1))\n",
    "\n",
    "# 4. Chargement du modele entraine\n",
    "clf = joblib.load('trained_logistic_regression.pkl')\n",
    "\n",
    "# 5. Prediction\n",
    "print('Prediction des tweets...')\n",
    "y_pred = clf.predict(X_test)\n",
    "df['sentiment'] = y_pred\n",
    "\n",
    "# 6. Sauvegarder les predictions\n",
    "# ajout de la ligne du candidat dans le dataframe\n",
    "for candidate in candidates:\n",
    "    curr_df = df[df[candidate]==True]\n",
    "    taille = curr_df.shape[0]\n",
    "    rec = {'count': taille, 'candidat': candidate}\n",
    "    try:\n",
    "        rec['neg'] = curr_df[curr_df['sentiment']==-1].shape[0] / taille\n",
    "        rec['neu'] = curr_df[curr_df['sentiment']==0].shape[0] / taille\n",
    "        rec['pos'] = curr_df[curr_df['sentiment']==1].shape[0] / taille\n",
    "    except:\n",
    "        # si aucun tweet pour le candidat courant n'est dans la base (cas pathologique)\n",
    "        rec['neg'], rec['neu'], rec['pos'] = ('-', '-', '-')\n",
    "    \n",
    "    df_pred = df_pred.append(rec, verify_integrity=False, ignore_index=True)\n",
    "df_pred.set_index('candidat', drop=True, inplace=True)\n",
    "print(df_pred)\n",
    "\n",
    "#print('Sauvegarde des pourcentages par candidat dans un .json : {}'.format(fname))\n",
    "#df_pred.to_json(fname)\n",
    "\n",
    "#print('Insertion dans la base MongoDB \"predicted\"...')\n",
    "#insert_in_mongo(df.drop(['other', 'count'], axis=1), port=27017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
